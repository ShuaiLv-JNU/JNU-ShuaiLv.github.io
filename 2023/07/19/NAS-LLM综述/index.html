<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Shuai Lv">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
        
            <link rel="preconnect" href="https://npm.elemecdn.com" crossorigin>
        
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://blogls.top/2023/07/19/nas-llm综述/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="1[计算、泛化问题-剪枝搜索空间]Ddpnas: Efficient neural architecture search via dynamic distribution pruning Zheng等人针对现有NAS方法计算开销大、泛化能力弱的问题,提出一种动态分布修剪策略(DDPNAS)来将操作选择建模为动态采样过程。并基于架构期望高效生成不同约束下的最优网络。作者从理论上证明了动态剪枝的有效">
<meta property="og:type" content="article">
<meta property="og:title" content="NAS-LLM综述">
<meta property="og:url" content="http://blogls.top/2023/07/19/NAS-LLM%E7%BB%BC%E8%BF%B0/index.html">
<meta property="og:site_name" content="乐愚良">
<meta property="og:description" content="1[计算、泛化问题-剪枝搜索空间]Ddpnas: Efficient neural architecture search via dynamic distribution pruning Zheng等人针对现有NAS方法计算开销大、泛化能力弱的问题,提出一种动态分布修剪策略(DDPNAS)来将操作选择建模为动态采样过程。并基于架构期望高效生成不同约束下的最优网络。作者从理论上证明了动态剪枝的有效">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-19T12:19:44.000Z">
<meta property="article:modified_time" content="2023-08-22T13:15:35.563Z">
<meta property="article:author" content="Shuai Lv">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
    
    
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q3ZMF4ZML1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-Q3ZMF4ZML1');
        </script>
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/favicon.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/favicon.svg">
    <!--- Page Info-->
    
    <title>
        
            NAS-LLM综述 -
        
        LeYuLiang
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/assets/fonts.css">
    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"blogls.top","root":"/","language":"en","path":"search.xml"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":true,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":true,"title":"推荐阅读","limit":3,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":true,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":true,"id":"G-Q3ZMF4ZML1"}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"LeYuLiang's Blog","subtitle":{"text":["丈夫处世兮，立功名","立功名兮，慰平生","慰平生兮，吾将醉","吾将醉兮，发狂吟"],"hitokoto":{"enable":true,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"links":{"github":"https://github.com/ShuaiLv-JNU","instagram":"https://www.instagram.com/liang_leyu/","zhihu":"https://www.zhihu.com/people/darker-7-73","twitter":"https://twitter.com/lushuai66337858","email":"lvshuai@stu2022.jnu.edu.cn"}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":true,"type":"fixed","audios":[{"name":"Something Just Like This","artist":"Coldplay","url":"https://evan.beee.top/music/Something%20Just%20Like%20This%20-%20The%20Chainsmokers%E3%80%81Coldplay.mp3","cover":"https://evan.beee.top/music/covers/Something_Just_Like_This.png"}]},"mermaid":{"enable":true,"version":"9.3.0"}},"version":"2.1.4","navbar":{"auto_hide":true,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Resources":{"icon":"fa-regular fa-folder","submenus":{"Photo":"/masonry"}},"About":{"icon":"fa-regular fa-user","submenus":{"Me":"/about","Github":"https://github.com/ShuaiLv-JNU"}},"Links":{"path":"/links","icon":"fa-regular fa-link"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":"You only live once, so make sense.","links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"},"Categories":{"path":"/categories","icon":"fa-regular fa-folder"}}},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}}};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    Global.data_config = {"masonry":true};
  </script>
    
    <!--- Fontawesome Part-->
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/fontawesome/fontawesome.min.css">
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/fontawesome/brands.min.css">
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/fontawesome/solid.min.css">
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/fontawesome/regular.min.css">
    
    
    
    
<meta name="generator" content="Hexo 5.4.2"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/rss2.xml" title="乐愚良" type="application/rss+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/logo.svg">
                </a>
            
            <a class="logo-title" href="/">
                
                LeYuLiang
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        ARCHIVES
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-folder"></i>
                                        
                                        RESOURCES&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a href="/masonry">PHOTO
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        ABOUT&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a href="/about">ME
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://github.com/ShuaiLv-JNU">GITHUB
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/links"  >
                                    
                                        
                                            <i class="fa-regular fa-link"></i>
                                        
                                        LINKS
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                ARCHIVES
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-folder"></i>
                                
                                RESOURCES&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="/masonry">PHOTO</a>
                            </li>
                        
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                ABOUT&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="/about">ME</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://github.com/ShuaiLv-JNU">GITHUB</a>
                            </li>
                        
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/links"  >
                             
                                
                                    <i class="fa-regular fa-link"></i>
                                
                                LINKS
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">NAS-LLM综述</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/avatar.jpg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Shuai Lv</span>
                            
                                <span class="author-label">Lv3</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2023-07-19 20:19:44</span>
        <span class="mobile">2023-07-19 20:19</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-08-22 21:15:35</span>
            <span class="mobile">2023-08-22 21:15</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/LLM/">LLM</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>15.9k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>57 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h3 id="1-计算、泛化问题-剪枝搜索空间-Ddpnas-Efficient-neural-architecture-search-via-dynamic-distribution-pruning"><a href="#1-计算、泛化问题-剪枝搜索空间-Ddpnas-Efficient-neural-architecture-search-via-dynamic-distribution-pruning" class="headerlink" title="1[计算、泛化问题-剪枝搜索空间]Ddpnas: Efficient neural architecture search via dynamic distribution pruning"></a>1[计算、泛化问题-剪枝搜索空间]<a class="link" target="_blank" rel="noopener" href="https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11263-023-01753-6&casa_token=-o8Kt_DhEy0AAAAA:QQGuYEPaArmGYsBVgTR-Jq2kT09VRcy2iGyWxHwrhJ-x2zPP7BCQj8U0OIcgoNsQDEEBQcnW41uP7V8YJw">Ddpnas: Efficient neural architecture search via dynamic distribution pruning <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Zheng等人针对现有NAS方法计算开销大、泛化能力弱的问题,提出一种动态分布修剪策略(DDPNAS)来将操作选择建模为动态采样过程。并基于架构期望高效生成不同约束下的最优网络。作者从理论上证明了动态剪枝的有效性,并在GLUE基准上验证了DDPNAS在压缩BERT搜索空间的可迁移性。</p>
<h3 id="2-泛化问题-NAS-MoE自适应-AutoMoE-Heterogeneous-Mixture-of-Experts-with-Adaptive-Computation-for-Efficient-Neural-Machine-Translation"><a href="#2-泛化问题-NAS-MoE自适应-AutoMoE-Heterogeneous-Mixture-of-Experts-with-Adaptive-Computation-for-Efficient-Neural-Machine-Translation" class="headerlink" title="2[泛化问题-NAS+MoE自适应]AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation"></a>2[泛化问题-NAS+MoE自适应]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/AutoMoE%3A-Heterogeneous-Mixture-of-Experts-with-for-Jawahar-Mukherjee/5165de3cd4f8dc9d88e82d55f4798013d57cc0f1">AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>混合专家（MoE）模型是一种稀疏激活的神经网络，能够有效利用大规模模型的参数。然而，大多数MoE模型采取了均匀设计(在不同的网络层中使用相同数量和大小的专家)。Jawahar等人通过AutoMoE框架考虑计算约束（如FLOPs，延迟）和输入的多样性，利用NAS技术在一个异构MoE的超网络中进行训练和进化搜索，并根据路由决策将不同输入分配给不同大小的专家，实现自适应计算。作者将AutoMoE设计的MoE模块嵌入到Transformer中来增强表示能力和泛化能力。</p>
<h3 id="3-弥合上下游差距-统一目标检测和语义分割-GAIA-Universe-Everything-is-Super-Netify-IEEE-Xplore"><a href="#3-弥合上下游差距-统一目标检测和语义分割-GAIA-Universe-Everything-is-Super-Netify-IEEE-Xplore" class="headerlink" title="3[弥合上下游差距-统一目标检测和语义分割]GAIA-Universe: Everything is Super-Netify - IEEE Xplore"></a>3[弥合上下游差距-统一目标检测和语义分割]<a class="link" target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10125046/">GAIA-Universe: Everything is Super-Netify - IEEE Xplore <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>为了弥合大规模上游数据预训练和下游任务定制化之间的鸿沟,Peng等人提出了一个统一的目标检测和语义分割框架。GAIA框架主要包括四个部分：1)从多个源收集数据并基于词向量相似度构建统一的标签空间,以消除数据集偏差;2）采用权重共享的超网络方式同时优化多种不同结构和规模的模型（Transformer、CNN以及混合架构）,覆盖不同的硬件和延迟需求;3）根据下游任务的领域和计算约束,设计了任务特定的架构选择机制,从超网络中搜索最优子网络;4）根据模型学习的特征空间距离进行任务特定的数据选择,以增强少样本场景下的性能。GAIA框架实现了从大规模预训练到下游任务定制化的有效过渡,在多个数据集上验证了其泛化性能。</p>
<h3 id="4-评估、弥合超子网差距问题-MoE控制子网权重共享程度-Mixture-of-Supernets-Improving-Weight-Sharing-Supernet-Training-with-Architecture-Routed-Mixture-of-Experts"><a href="#4-评估、弥合超子网差距问题-MoE控制子网权重共享程度-Mixture-of-Supernets-Improving-Weight-Sharing-Supernet-Training-with-Architecture-Routed-Mixture-of-Experts" class="headerlink" title="4[评估、弥合超子网差距问题-MoE控制子网权重共享程度]Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts"></a>4[评估、弥合超子网差距问题-MoE控制子网权重共享程度]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.04845">Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>现有的NAS方法面临两个挑战分别是候选架构重复训练和评估的低效性、超网络与单独训练模型性能之间的差异性。Jawahar等人提出混合超网络（MoS）框架，通过层级和神经元级的混合专家（MoE）机制自适应地控制子网络之间权重共享程度。作者通过优化BERT的超网络训练、子网络搜索与机器翻译来验证了MoS框架的有效性。</p>
<h3 id="5-高效的PLM微调-剪枝搜索空间？-Neural-Architecture-Search-for-Parameter-Efficient-Fine-tuning-of-Large-Pre-trained-Language-Models"><a href="#5-高效的PLM微调-剪枝搜索空间？-Neural-Architecture-Search-for-Parameter-Efficient-Fine-tuning-of-Large-Pre-trained-Language-Models" class="headerlink" title="5[高效的PLM微调-剪枝搜索空间？]Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models"></a>5[高效的PLM微调-剪枝搜索空间？]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.16597">Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>预训练语言模型(PLM)的参数量巨大，手工设计的参数高效微调(PET)通过只调整PLM参数的一个子集来实现下游任务的微调。Lawton等人提出基于剪枝的NAS方法来自动发现参数高效微调（PET）架构。具体来说，设计了包含结构化或非结构化的偏置微调和低秩适应（LoRA）的搜索空间，以训练损失函数的一阶近似作为剪枝标准，并对剪枝后的PET架构进行重新初始化和重新训练，遵循了彩票假设（lottery ticket hypothesis）。作者在GLUE基准上验证了算法的有效性，并分析了不同的PET架构设计选择对参数效率的影响。</p>
<h3 id="6-性能问题-NAS-图-Autogt-Automated-graph-transformer-architecture-search"><a href="#6-性能问题-NAS-图-Autogt-Automated-graph-transformer-architecture-search" class="headerlink" title="6[性能问题-NAS+图]Autogt: Automated graph transformer architecture search"></a>6[性能问题-NAS+图]<a class="link" target="_blank" rel="noopener" href="https://openreview.net/forum?id=GcM7qfl5zY">Autogt: Automated graph transformer architecture search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>目前的图Transformer设计仍然依赖于手工设计的神经架构和图编码策略，Zhang等人首次提出图的自动化Transformer架构搜索方法AutoGT，设计了统一的图Transformer搜索空间，包含Transformer架构和图编码策略，并根据图编码策略的关联度逐步训练和拆分超网，从而更准确地评估联合优化的架构和编码效果。AutoGT在多个图分类数据集上的性能均优于手工设计。</p>
<h3 id="7-操作和架构的表示能力-NAR-Former-Neural-Architecture-Representation-Learning-towards-Holistic-Attributes-Prediction"><a href="#7-操作和架构的表示能力-NAR-Former-Neural-Architecture-Representation-Learning-towards-Holistic-Attributes-Prediction" class="headerlink" title="7[操作和架构的表示能力]NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction"></a>7[操作和架构的表示能力]<a class="link" target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Yi_NAR-Former_Neural_Architecture_Representation_Learning_Towards_Holistic_Attributes_Prediction_CVPR_2023_paper.html">NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>现有的表示学习方法存在信息损失、维度爆炸和长距离依赖等问题，Yi等人提出一个通用的神经网络结构表示学习框架,用于预测不同神经网络结构的属性，如准确率和延迟。具体来说，NAR-Former设计了一个分词器将神经网络的操作和拓扑信息编码为纯序列，以位置嵌入表示实数值，然后提出了一个多阶段融合Transformer，利用自注意力机制从序列中学习紧凑的向量表示，并通过信息流一致性增强方法和对应的结构一致性损失函数来提高表示学习效果。实验结果表明，该框架能够在下游预测任务中提高准确性。</p>
<h3 id="8-眼动估计问题-HRNAS的改进-Searching-Efficient-Neural-Architecture-with-Multi-resolution-Fusion-Transformer-for-Appearance-based-Gaze-Estimation"><a href="#8-眼动估计问题-HRNAS的改进-Searching-Efficient-Neural-Architecture-with-Multi-resolution-Fusion-Transformer-for-Appearance-based-Gaze-Estimation" class="headerlink" title="8[眼动估计问题-HRNAS的改进]Searching Efficient Neural Architecture with Multi-resolution Fusion Transformer for Appearance-based Gaze Estimation"></a>8[眼动估计问题-HRNAS的改进]<a class="link" target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/WACV2023/html/Nagpure_Searching_Efficient_Neural_Architecture_With_Multi-Resolution_Fusion_Transformer_for_Appearance-Based_WACV_2023_paper.html">Searching Efficient Neural Architecture with Multi-resolution Fusion Transformer for Appearance-based Gaze Estimation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Nagpure等人提出一种精确而高效的基于外观的注视估计方法。为此，使用NAS方法设计了一个多分辨率特征提取器,可以同时提取全局和局部特征信息，以及一个一个多分辨率融合Transformer作为回归头,可以有效融合不同分辨率的特征图,精确预测凝视值。作者在ETH-XGaze数据集上搜索了一个适用于注视估计任务的神经架构，称为GazeNAS-ETH，并在其他四个公开数据集上对其进行了评估。实验结果表明搜索到的GazeNAS-ETH模型参数更少、计算量更小，但精度更高。</p>
<h3 id="9-NAS搜索空间加入先验-GPT-NAS-Neural-Architecture-Search-with-the-Generative-Pre-Trained-Model"><a href="#9-NAS搜索空间加入先验-GPT-NAS-Neural-Architecture-Search-with-the-Generative-Pre-Trained-Model" class="headerlink" title="9[NAS搜索空间加入先验]GPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model"></a>9[NAS搜索空间加入先验]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.05351">GPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Yu等人利用GPT模型的生成能力和对语言规律的学习能力来学习神经网络架构的基本规律,从而提供先验知识指导NAS算法的搜索来减小搜索空间。具体来说,GPT-NAS将神经网络架构编码为文本,利用大量神经网络架构数据预训练GPT模型,再微调到具体NAS任务中,并引入进化算法进行网络架构搜索。此外，作者利用微调后的GPT模型对样例网络架构进行再生成和重构来优化架构。作者在不同的图像分类任务上验证了该算法的优越性。</p>
<h3 id="10-低资源-混合搜索空间-HyT-NAS-Hybrid-Transformers-Neural-Architecture-Search-for-Edge-Devices"><a href="#10-低资源-混合搜索空间-HyT-NAS-Hybrid-Transformers-Neural-Architecture-Search-for-Edge-Devices" class="headerlink" title="10[低资源-混合搜索空间]HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices"></a>10[低资源-混合搜索空间]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.04440">HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Mecharbat等人提出了一种针对边缘和微型设备的硬件感知NAS方法，称为HyT-NAS。该方法设计了混合注意力和卷积块的搜索空间，采用了改进的多目标贝叶斯优化搜索策略，并使用定制的性能评估预测器来提高搜索效率和质量。在目标检测实验中验证了搜索到骨干网络的优越性。</p>
<h3 id="11-统一下游任务-多任务-MDL-NAS-A-Joint-Multi-Domain-Learning-Framework-for-Vision-Transformer"><a href="#11-统一下游任务-多任务-MDL-NAS-A-Joint-Multi-Domain-Learning-Framework-for-Vision-Transformer" class="headerlink" title="11[统一下游任务-多任务]MDL-NAS: A Joint Multi-Domain Learning Framework for Vision Transformer"></a>11[统一下游任务-多任务]<a class="link" target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/CVPR2023/html/Wang_MDL-NAS_A_Joint_Multi-Domain_Learning_Framework_for_Vision_Transformer_CVPR_2023_paper.html">MDL-NAS: A Joint Multi-Domain Learning Framework for Vision Transformer <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>视觉Transformer在多任务的场景下需要不同的编码器来管理不同的域，导致存储效率低下。Wang等人提出了MDL-NAS框架,通过粗细搜索空间设计和子网联合搜索算法,找到每个任务的最优架构,使不同任务之间参数共享尽可能多以提升存储效率。具体来说，MDL-NAS构建了一个由粗到细的搜索空间，其中粗搜索空间提供了不同任务的最优架构，而细搜索空间提供了细粒度的参数共享，包括顺序共享策略和掩码共享策略，实现了每层网络中部分参数共享、部分参数独享。最后，MDL-NAS提出了一个子网联合搜索算法,在总资源约束下找到每个任务的最优架构和共享参数。</p>
<h3 id="12-NAS排名-PerfHD-Efficient-ViT-Architecture-Performance-Ranking-…"><a href="#12-NAS排名-PerfHD-Efficient-ViT-Architecture-Performance-Ranking-…" class="headerlink" title="12[NAS排名]PerfHD: Efficient ViT Architecture Performance Ranking …"></a>12[NAS排名]<a class="link" target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023W/NAS/papers/Ma_PerfHD_Efficient_ViT_Architecture_Performance_Ranking_Using_Hyperdimensional_Computing_CVPRW_2023_paper.pdf">PerfHD: Efficient ViT Architecture Performance Ranking … <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>NAS中评估候选架构的性能往往需要训练和对大量架构进行排名，Ma等人使用超维计算(HDC)来实现高效而准确的ViT架构性能排名，称为PerfHD。该方法使用两种HDC编码方案基于Gram及基于Record来将ViT参数编码为超向量（HV），其中，基于Record编码使用深度内存作为替代品，以消除基于Gram的置换操作。随后使用权重和HV进行加权和以建立基于训练集中的性能排名关联内存，并提出了基于权重更新的再训练策略。在VIMER-UFO基准测试中，PerfHD可以在1分钟内对近10万个ViT模型进行排序,速度更快，准确率更高。</p>
<h3 id="13-混合架构-无需训练-设计指标-AutoST-Training-free-Neural-Architecture-Search-for-Spiking-Transformers"><a href="#13-混合架构-无需训练-设计指标-AutoST-Training-free-Neural-Architecture-Search-for-Spiking-Transformers" class="headerlink" title="13[混合架构-无需训练-设计指标]AutoST: Training-free Neural Architecture Search for Spiking Transformers"></a>13[混合架构-无需训练-设计指标]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/AutoST%3A-Training-free-Neural-Architecture-Search-Wang-Zhao/c0074d576c440fd0f2d2e1371b55e359a2043175">AutoST: Training-free Neural Architecture Search for Spiking Transformers <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Spiking Transformer是一种结合了脉冲神经网络（SNNs）和Transformer的模型，具有节能和高容量的优势。然而，现有的人工设计或者基于训练的NAS方法难以兼顾性能和能耗。Wang等人提出了无需训练的NAS方法AutoST，一种基于浮点运算（FLOPs）和激活模式（activation patterns）的评估指标，来快速地从初始化的网络中筛选出最优的Spiking Transformer架构。在静态和神经形态数据集上的结果表明，AutoST模型在准确率和能耗方面均优于现有的模型。</p>
<h3 id="14-架构表示能力-PINAT-A-Permutation-INvariance-Augmented-Transformer-for-NAS-Predictor-Transformer用于NAS"><a href="#14-架构表示能力-PINAT-A-Permutation-INvariance-Augmented-Transformer-for-NAS-Predictor-Transformer用于NAS" class="headerlink" title="14[架构表示能力]PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor(Transformer用于NAS)"></a>14[架构表示能力]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/PINAT%3A-A-Permutation-INvariance-Augmented-for-NAS-Lu-Hu/3930a0285877386fca94e60c4c69305e220fd4ee">PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>(Transformer用于NAS)</h3><p>针对NAS中性能预测任务难以建模架构的拓扑信息的问题，Lu等人设计了基于Transformer的NAS预测器PINAT，它包含了一个部分置换不变增强模块（PIM），用于对架构图的节点特征进行局部和全局的置换不变编码，并使用拉普拉斯矩阵作为位置编码，来捕捉架构图的拓扑信息。作者在六个公开的搜索空间上进行了实验，PINAT在排名性能、搜索效率和搜索质量方面都超过了最先进的NAS预测器方法。</p>
<h3 id="15-无需训练-设计指标-Training-free-Neural-Architecture-Search-for-RNNs-and-Transformers"><a href="#15-无需训练-设计指标-Training-free-Neural-Architecture-Search-for-RNNs-and-Transformers" class="headerlink" title="15[无需训练-设计指标]Training-free Neural Architecture Search for RNNs and Transformers"></a>15[无需训练-设计指标]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Training-free-Neural-Architecture-Search-for-RNNs-Serianni-Kalita/6cd94eee6bb0d10e095af1a297919fc73c636297">Training-free Neural Architecture Search for RNNs and Transformers <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>传统的NAS算法需要大量的计算资源和时间来训练和评估候选架构，Serianni等人提出了一种无需训练就可以预测架构准确性的指标，包括针对RNN的新指标hidden covariance、针对BERT（基于Transformer编码器）的基于注意力头剪枝的三个指标Attention Confidence、Softmax Confidence和Importance。作者在NAS-Bench-NLP和BERT NAS基准上测试后发现隐藏协方差在RNN上表现最好，而注意力置信度在Transformer上表现最好。此外，由于Transformer模型大小对性能的影响过大，因此需要联合设计搜索空间和指标。</p>
<h3 id="16-缩小搜索空间-PreNAS-Preferred-One-Shot-Learning-Towards-Efficient-Neural-Architecture-Search"><a href="#16-缩小搜索空间-PreNAS-Preferred-One-Shot-Learning-Towards-Efficient-Neural-Architecture-Search" class="headerlink" title="16[缩小搜索空间]PreNAS: Preferred One-Shot Learning Towards Efficient Neural Architecture Search"></a>16[缩小搜索空间]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/PreNAS%3A-Preferred-One-Shot-Learning-Towards-Neural-Wang-Ge/f7c9191c653ed6ab7219c3a1b24350ab2af5e3f8">PreNAS: Preferred One-Shot Learning Towards Efficient Neural Architecture Search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>传统的one-shot NAS方法需要在一个巨大的搜索空间中训练和评估大量的子网，Wang等人提出了优选学习（preferred learning），它可以在训练前通过零成本代理（zero-cost proxy）预先筛选出高质量的架构，然后在这个较小的优先搜索空间内进行聚焦的one-shot训练。此外，设计了一个复合的架构选择机制，利用一个修正过的SNIP分数来消除Transformer架构中的”isomers”问题，提出了一种平衡性能分布的方法，通过对不同层次和维度的架构进行分组采样来提高不同子网络的训练公平性。实验结果证明了PreNAS优秀的性能和搜索效率。</p>
<h3 id="17-超网与子网的差距问题-梯度冲突-ElasticViT-Conflict-aware-Supernet-Training-for-Deploying-Fast-Vision-Transformer-on-Diverse-Mobile-Devices"><a href="#17-超网与子网的差距问题-梯度冲突-ElasticViT-Conflict-aware-Supernet-Training-for-Deploying-Fast-Vision-Transformer-on-Diverse-Mobile-Devices" class="headerlink" title="17[超网与子网的差距问题-梯度冲突]ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices"></a>17[超网与子网的差距问题-梯度冲突]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/ElasticViT%3A-Conflict-aware-Supernet-Training-for-on-Tang-Zhang/473f52a246b194431b6deaf1f0e64ece0c8e80f9">ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>NAS在一个包含tiny模型和large模型的巨大搜索空间中会遇到gradient conflict问题，导致supernet性能下降。Tang等人提出两种创新的采样技术：复杂度感知采样和性能感知采样。complexity-aware sampling限制了相邻训练步骤中采样子网络的FLOPs差异,同时覆盖不同大小的子网络；performance-aware sampling利用一个基于记忆库和路径偏好规则的探索和利用策略，优先采样潜在准确率较高的子网络。论文在ImageNet数据集上进行了广泛的实验，发现ElasticViT在不同移动设备上都能超越现有的高效CNN和ViT模型，达到更高的准确率和更快的速度。</p>
<h3 id="18-自适应量化-Mixed-precision-quantization-of-transformer-language-models-for-speech-recognition"><a href="#18-自适应量化-Mixed-precision-quantization-of-transformer-language-models-for-speech-recognition" class="headerlink" title="18[自适应量化]Mixed precision quantization of transformer language models for speech recognition"></a>18[自适应量化]<a class="link" target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9414076/">Mixed precision quantization of transformer language models for speech recognition <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>当前统一精度的低位量化方法无法适应不同Transformer模块对量化误差的不同敏感性，Xu等人提出自动学习每个模块的局部量化精度，其方法有两种：一是基于量化敏感度指标（量化扰动的海森矩阵迹加权）,二是混合精度Transformer网络架构搜索，并使用替代方向乘子方法训练混合精度量化模型。实验在Penn Treebank和Switchboard语音识别任务上表明,与统一精度量化相比,混合精度量化可以在无性能损失的情况下实现16倍模型压缩。 </p>
<h3 id="19-加入先验-架构、数据表达能力-Transfer-NAS-with-Meta-learned-Bayesian-Surrogates"><a href="#19-加入先验-架构、数据表达能力-Transfer-NAS-with-Meta-learned-Bayesian-Surrogates" class="headerlink" title="19[加入先验-架构、数据表达能力]Transfer NAS with Meta-learned Bayesian Surrogates"></a>19[加入先验-架构、数据表达能力]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Transfer-NAS-with-Meta-learned-Bayesian-Surrogates-Shala-Elsken/05a4880bc44ee91b846541a330ba5aa7a874cdf0">Transfer NAS with Meta-learned Bayesian Surrogates <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Shala等人结合黑盒优化的可靠性以及one-shot NAS方法的高效性，利用贝叶斯优化（BO）和深度核高斯过程（GP）来学习和转移优秀的神经架构。具体来说，作者将NAS表述为一个迁移学习问题,通过元学习一个高斯过程回归模型来预测架构性能,该模型使用图神经网络（GNN）和Transformer来分别编码神经架构和数据集特征，并在测试时通过贝叶斯优化迭代更新模型。作者在六个计算机视觉数据集上验证了方法的先进性。</p>
<h3 id="20-无需训练-网络复杂度指标-2202-11921-Auto-scaling-Vision-Transformers-without-Training-…"><a href="#20-无需训练-网络复杂度指标-2202-11921-Auto-scaling-Vision-Transformers-without-Training-…" class="headerlink" title="20[无需训练-网络复杂度指标][2202.11921] Auto-scaling Vision Transformers without Training …"></a>20[无需训练-网络复杂度指标][<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.11921">2202.11921] Auto-scaling Vision Transformers without Training … <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>针对手动设计和缩放ViT存在效率低下的问题，Chen等人提出了一个无需训练的自动设计与扩展ViT的统一框架As-ViT，它利用网络复杂度度量来指导无训练的拓扑搜索和缩放规则，从而快速有效地发现和扩展ViT，并提出了一种渐进式重标记化策略，通过改变采样粒度来动态调整ViT的输入令牌，从而实现高效的ViT训练。作者在ImageNet-1k分类和COCO检测上验证了As-ViT的先进性。</p>
<h3 id="21-低资源-混合架构-Alternative-non-BERT-model-choices-for-the-textual-classification-in-low-resource-languages-and-environments"><a href="#21-低资源-混合架构-Alternative-non-BERT-model-choices-for-the-textual-classification-in-low-resource-languages-and-environments" class="headerlink" title="21[低资源-混合架构]Alternative non-BERT model choices for the textual classification in low-resource languages and environments"></a>21[低资源-混合架构]<a class="link" target="_blank" rel="noopener" href="https://aclanthology.org/2022.deeplo-1.20.pdf">Alternative non-BERT model choices for the textual classification in low-resource languages and environments <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>针对低资源语言和环境中的文本分类任务,Maheen等人提出了一种基于CNN和RNN的混合模型结构,即融合链（fusion chain）模型,来解决BERT等大型预训练模型的计算资源需求过高的问题。作者认为不同深度神经网络层之间的融合可以整合局部和全局文本依赖信息,并通过NAS确定了最佳的融合链长度和层次顺序。实验结果表明,简单的融合链模型可以在多种低资源语言文本分类任务中取得与BERT类似或者更好的性能,同时参数量和计算量大大减少。</p>
<h3 id="22-剪枝-多目标-SwiftPruner-Reinforced-Evolutionary-Pruning-for-Efficient-Ad-Relevance"><a href="#22-剪枝-多目标-SwiftPruner-Reinforced-Evolutionary-Pruning-for-Efficient-Ad-Relevance" class="headerlink" title="22[剪枝-多目标]SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance"></a>22[剪枝-多目标]<a class="link" target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3511808.3557139?casa_token=4wraNNUImqIAAAAA:QbFaQixIrlfRVPEvJl2ZSeh4P-l3dzag2J5XwInFsz3OjkLcIo40bab3JCyc6p0YMQ7Q8J721PeaNg">SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>为了解决在线广告系统中新的冷启动广告无法获得离线预计算的广告端表示的问题，Zhang等人提出SwiftPruner框架，利用NAS技术自动搜索在给定延迟约束下表现最佳的层次稀疏BERT模型，并引入了一个强化进化算法，通过一个考虑准确性和延迟的多目标奖励函数，学习进行更好的稀疏变异。在大规模真实数据集上的试验表明，SwiftBERT模型达到更高的AUC和更低的延迟，部署到在线系统后能大大降低冷启动广告的缺陷率。</p>
<h3 id="23-PLM-NAS-混合搜索空间-重用参数-Autobert-zero-Evolving-bert-backbone-from-scratch"><a href="#23-PLM-NAS-混合搜索空间-重用参数-Autobert-zero-Evolving-bert-backbone-from-scratch" class="headerlink" title="23[PLM+NAS-混合搜索空间-重用参数]Autobert-zero: Evolving bert backbone from scratch"></a>23[PLM+NAS-混合搜索空间-重用参数]<a class="link" target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/21311">Autobert-zero: Evolving bert backbone from scratch <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>预训练语言模型(PLM)骨干结构中手工设计的自注意力机制存在冗余参数和诱导偏置，Gao等人设计了一个分层的粗细搜索空间,内层为基础数学运算构建的灵活自注意力结构，而外层为全局注意力层与局部卷积层的组合，提出了一种操作优先级NAS进化算法,通过平衡探索和利用来提高搜索效率，设计了双分支权重共享训练策略,通过重用先前训练的参数加速模型评估。在多个自然语言理解和问答任务上的实验表明,最好的搜索出的模型AutoBERT-Zero显著优于BERT和其变体。</p>
<h3 id="24-权重共享导致的排名问题-剪枝-架构表达能力-Analyzing-and-mitigating-interference-in-neural-architecture-search"><a href="#24-权重共享导致的排名问题-剪枝-架构表达能力-Analyzing-and-mitigating-interference-in-neural-architecture-search" class="headerlink" title="24[权重共享导致的排名问题-剪枝-架构表达能力]Analyzing and mitigating interference in neural architecture search"></a>24[权重共享导致的排名问题-剪枝-架构表达能力]<a class="link" target="_blank" rel="noopener" href="https://proceedings.mlr.press/v162/xu22h.html">Analyzing and mitigating interference in neural architecture search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>NAS中权重共享会导致的不同子模型之间的干扰，进而影响子模型的排名和性能评估。Xu等人从两个角度提出了两种减轻干扰的方法：MAGIC-T和MAGIC-A。MAGIC-T通过逐渐修改子模型的拓扑结构，使得相邻采样步骤之间的拓扑变化最小，从而减少共享操作器上的干扰。MAGIC-A通过选择一个表现优异的锚定子模型，使得其他子模型的共享操作器的输入和输出与锚定子模型保持一致，从而对齐不同子模型的优化方向。作者在BERT预训练、压缩以及ImageNet图像分类任务上验证了方法的有效性和泛化性。</p>
<h3 id="25-文本识别-渐进式-Searching-a-high-performance-feature-extractor-for-text-recognition-network"><a href="#25-文本识别-渐进式-Searching-a-high-performance-feature-extractor-for-text-recognition-network" class="headerlink" title="25[文本识别-渐进式]Searching a high performance feature extractor for text recognition network"></a>25[文本识别-渐进式]<a class="link" target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9887897/">Searching a high performance feature extractor for text recognition network <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>目前文本识别系统直接沿用其他任务设计的特征提取器结构,没有考虑文本识别任务的特点，zhang等人提出了一个面向文本识别任务的搜索空间,空间模型用于搜索卷积类型和下采样路径，时序模型用于搜索Transformer的变体,并设计了一个两阶段的搜索算法,第一阶段渐进式训练超网络，第二阶段从超网络中搜索一个满足延迟约束的子网络。实验结果在各类手写体和场景文本数据集上都取得了SOTA结果。</p>
<h3 id="26-渐进式-Automated-progressive-learning-for-efficient-training-of-vision-transformers"><a href="#26-渐进式-Automated-progressive-learning-for-efficient-training-of-vision-transformers" class="headerlink" title="26[渐进式]Automated progressive learning for efficient training of vision transformers"></a>26[渐进式]<a class="link" target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/CVPR2022/html/Li_Automated_Progressive_Learning_for_Efficient_Training_of_Vision_Transformers_CVPR_2022_paper.html">Automated progressive learning for efficient training of vision transformers <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Li等人提出了一种自动化的渐进式学习（AutoProg）方法实现无损的训练加速。论文设计了一个强大的手动基线,通过引入momentum growth策略进行渐进式学习；提出了一种自动的渐进式学习方法(AutoProg),将增长进度表优化问题放松为子网络结构优化，并通过训练一个弹性超网来进行子网络性能的一次性估计。实验结果表明,AutoProg可以显著减少DeiT和VOLO等多个视觉transformer模型在ImageNet上的训练时间，同时保持可比的性能。</p>
<h3 id="27-低资源-混合搜索空间-异步权重共享-ShiftAddNAS-Hardware-inspired-search-for-more-accurate-and-efficient-neural-networks"><a href="#27-低资源-混合搜索空间-异步权重共享-ShiftAddNAS-Hardware-inspired-search-for-more-accurate-and-efficient-neural-networks" class="headerlink" title="27[低资源-混合搜索空间-异步权重共享]ShiftAddNAS: Hardware-inspired search for more accurate and efficient neural networks"></a>27[低资源-混合搜索空间-异步权重共享]<a class="link" target="_blank" rel="noopener" href="https://proceedings.mlr.press/v162/you22a.html">ShiftAddNAS: Hardware-inspired search for more accurate and efficient neural networks <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>You等人针对移位和加法等低成本运算符的网络提出了ShiftAddNAS框架，该框架设计了第一个同时包含乘法（卷积、自注意力）和非乘法算子（移位和加法）的混合搜索空间，并提出了一个异构权重共享策略，在共享权重的同时保持各自的分布特征。作者在多个自然语言处理和计算机视觉任务上验证了ShiftAddNAS的有效性。</p>
<h3 id="28-无需训练-突触多样性和显著性作为指标-Training-free-transformer-architecture-search"><a href="#28-无需训练-突触多样性和显著性作为指标-Training-free-transformer-architecture-search" class="headerlink" title="28[无需训练-突触多样性和显著性作为指标]Training-free transformer architecture search"></a>28[无需训练-突触多样性和显著性作为指标]<a class="link" target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Training-Free_Transformer_Architecture_Search_CVPR_2022_paper.html">Training-free transformer architecture search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Zhou等人提出了基于多头自注意力模块的突触多样性和基于多层感知机模块的突触显著性来作为无需训练的指标来评估不同的Transformer架构。实验结果表明,该方法只需要0.5个GPU天就可以搜索出性能竞争的Transformer模型,相比现有方法提高了近48倍的搜索效率。</p>
<h3 id="29-最优子网-Vision-transformer-slimming-Multi-dimension-searching-in-continuous-optimization-space"><a href="#29-最优子网-Vision-transformer-slimming-Multi-dimension-searching-in-continuous-optimization-space" class="headerlink" title="29[最优子网]Vision transformer slimming: Multi-dimension searching in continuous optimization space"></a>29[最优子网]<a class="link" target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2022/html/Chavan_Vision_Transformer_Slimming_Multi-Dimension_Searching_in_Continuous_Optimization_Space_CVPR_2022_paper.html?ref=roboflow-blog">Vision transformer slimming: Multi-dimension searching in continuous optimization space <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Chavan等人探索了从视觉transformer中找到一个最优子模型的可行性，并提出了一个纯vision transformer slimming (ViT-Slim) 框架，它可以跨多个维度从原始模型中端到端搜索一个子结构,包括输入标记、MHSA和MLP模块,实现了state-of-the-art的性能。ViT-Slim基于一个可学习的统一的L1稀疏约束,定义了不同维度上的全局重要性因子,以反映连续搜索空间中不同维度的全局重要性，并且搜索过程通过单次训练方案非常高效。大量的实验表明了ViT-Slim搜索到的模型在ImageNet和下游数据集上的优势。</p>
<h3 id="30-剪枝-多目标进化-权重重构-EAPruning-Evolutionary-Pruning-for-Vision-Transformers-and-CNNs"><a href="#30-剪枝-多目标进化-权重重构-EAPruning-Evolutionary-Pruning-for-Vision-Transformers-and-CNNs" class="headerlink" title="30[剪枝-多目标进化-权重重构]EAPruning: Evolutionary Pruning for Vision Transformers and CNNs"></a>30[剪枝-多目标进化-权重重构]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/EAPruning:-Evolutionary-Pruning-for-Vision-and-CNNs-Li-Zhang/2fe23874ed2bf268bd35b787487b7f70a2c78377">EAPruning: Evolutionary Pruning for Vision Transformers and CNNs <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>目前的剪枝方法需要深入的领域知识，且只适用于某种特定类型的网络，Li等人认为剪枝是一种在有限空间中寻找最优子网络结构的过程，提出了一个通用的基于多目标进化算法NSGA-III的神经网络剪枝方法EAPruning,对视觉Transformer和卷积网络使用粗粒度的搜索空间编码，并应用权重重构进行子网络快速评估。实验结果表明，该方法在DeiT、MobileNet等模型都能达到显著的压缩和加速效果。</p>
<h3 id="31-搜索空间-Searching-for-BurgerFormer-with-micro-meso-macro-space-design"><a href="#31-搜索空间-Searching-for-BurgerFormer-with-micro-meso-macro-space-design" class="headerlink" title="31[搜索空间]Searching for BurgerFormer with micro-meso-macro space design"></a>31[搜索空间]<a class="link" target="_blank" rel="noopener" href="https://proceedings.mlr.press/v162/yang22f.html">Searching for BurgerFormer with micro-meso-macro space design <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Yang等人设计一个新的搜索空间，从微观、中观和宏观三个层面来搜索高性能的类Transformer结构，命名为BurgerFormer。在微观粒度,丰富了原子操作的选择,包含不同的正则化、激活函数和基本运算；在中观粒度,提出汉堡包结构的块设计,包含输入输出的 Norm-Op-Norm-Act 以及中间的反瓶颈结构；在宏观粒度,搜索多阶段的视觉Transformer的宽度、深度和扩张比例，并提出混合取样策略来有效训练超网。作者在ImageNet和COCO数据集上的实验结果验证了搜索空间设计的有效性以及搜索到模型的可迁移性。</p>
<h3 id="32-统一-UFO-unified-feature-optimization"><a href="#32-统一-UFO-unified-feature-optimization" class="headerlink" title="32[统一]UFO: unified feature optimization"></a>32[统一]<a class="link" target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-19809-0_27">UFO: unified feature optimization <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>为了利用大规模的多任务预训练来提高下游任务的性能，Xi等人提出了一种新的训练和部署范式，叫做统一特征优化（UFO），具有相对较小的模型大小和无需适配成本的特性。具体来说，UFO将各种任务压缩到一个适度大小的统一模型中进行多任务学习，进一步在转移到下游任务时裁剪模型大小（UFO将裁剪视为NAS问题）；UFO不强调转移到新任务,而是直接从已见的统一模型中选择部分模块以完全无适配成本的方式部署到已经见过的下游子任务中。在多个深度表示学习任务上的实验表明,从超网裁剪得到的子模型优于单任务训练的对应模型。</p>
<h3 id="33-统一-Uninet-Unified-architecture-search-with-convolution-transformer-and-mlp"><a href="#33-统一-Uninet-Unified-architecture-search-with-convolution-transformer-and-mlp" class="headerlink" title="33[统一]Uninet: Unified architecture search with convolution, transformer, and mlp"></a>33[统一]<a class="link" target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-19803-8_3">Uninet: Unified architecture search with convolution, transformer, and mlp <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Liu等人提出了一个统一的架构搜索方法,可以联合搜索卷积、transformer和MLP来构建高性能的混合视觉网络架构。统一的NAS方法包括了两个关键设计：一是用统一的形式建模不同的操作符,使它们可以用相同的参数集进行特征化,从而大大减小了搜索空间，二是提出了结合局部和全局上下文的下采样模块,以更好地传递不同操作符之间的特征。作者在ImageNet数据集上搜索得到了UniNet架构,然后通过扩展网络规模得到了UniNet模型族,在图像分类、目标检测和语义分割任务上都优于之前的卷积或transformer网络,证明了所提出方法的有效性。</p>
<h3 id="34-无需训练-LiteTransformerSearch-Training-free-Neural-Architecture-Search-for-Efficient-Language-Models"><a href="#34-无需训练-LiteTransformerSearch-Training-free-Neural-Architecture-Search-for-Efficient-Language-Models" class="headerlink" title="34[无需训练]LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models"></a>34[无需训练]<a class="link" target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/9949e6906be6448230cdba9a4cb2d564-Abstract-Conference.html">LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Javaheripi等人提出了一个轻量级的Transformer搜索方法，用于无需训练就可以高效地搜索出在困惑度与硬件性能(延迟和内存)之间实现Pareto最优架构。该方法利用了decoder参数数量与最终模型困惑度之间存在高相关性的经验发现,提供了一个零成本的代理指标来估计模型困惑度,避免了搜索过程中的昂贵的模型训练。实验结果显示,在WikiText-103和LM1B数据集上,基于decoder参数数量的代理指标可以提供比部分训练更准确的困惑度估计，搜索到的模型在下游NLP任务上的zero-shot和one-shot性能也优于手工设计。</p>
<h3 id="35-Searching-for-Better-Spatio-temporal-Alignment-in-Few-Shot-Action-Recognition"><a href="#35-Searching-for-Better-Spatio-temporal-Alignment-in-Few-Shot-Action-Recognition" class="headerlink" title="35[]Searching for Better Spatio-temporal Alignment in Few-Shot Action Recognition"></a>35[]<a class="link" target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/8693ee1ea821666f8569228d1ab38baf-Abstract-Conference.html">Searching for Better Spatio-temporal Alignment in Few-Shot Action Recognition <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>在少样本视频动作识别任务中,存在空间-时域特征匹配和对齐的问题,Cao等人从模型结构的角度，利用NAS技术自动搜索时空注意力模块的组合方式，并提出transformer空间收缩策略,以加速supernet的训练和结构搜索。此外，设计了一种非参数化的空间-时域原型对齐策略，针对每个查询视频和类别，生成一个与所有支持视频匹配的查询特定类原型，以应对动作的高变化性。在UCF101和HMDB51数据集上,该方法相比基准方法获得了显著提升。</p>
<h3 id="36-Autotransformer-Automatic-transformer-architecture-design-for-time-series-classification"><a href="#36-Autotransformer-Automatic-transformer-architecture-design-for-time-series-classification" class="headerlink" title="36[]Autotransformer: Automatic transformer architecture design for time series classification"></a>36[]<a class="link" target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-05933-9_12">Autotransformer: Automatic transformer architecture design for time series classification <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Ren等人针对时间序列分类任务,提出了一个名为AutoTransformer的神经架构搜索方法,来自动设计适合不同时间序列数据集的神经网络架构。具体来说，作者提出了一个针对时间序列任务定制的搜索空间,包含了卷积、循环神经网络、自注意力等不同模块,以模拟时间序列中的局部和全局特征。在搜索算法上，增加了层输入选择和残差连接选择,使搜索空间更丰富。在UCR时间序列分类benchmark数据集上的实验表明,AutoTransformer可以为不同的数据集自动搜索到适合的模型架构,并取得了state-of-the-art的结果。</p>
<h3 id="37-情绪识别-EEG-based-emotion-recognition-via-transformer-neural-architecture-search"><a href="#37-情绪识别-EEG-based-emotion-recognition-via-transformer-neural-architecture-search" class="headerlink" title="37[情绪识别]EEG-based emotion recognition via transformer neural architecture search"></a>37[情绪识别]<a class="link" target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9763316/">EEG-based emotion recognition via transformer neural architecture search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Li等人将基于Transformer的NAS方法(TNAS)应用到脑电图的情感识别任务，根据识别精度和模型复杂度两个目标函数，利用多目标进化算法来自动搜索出任务匹配的Transformer架构。在DEAP和DREAMER两个数据集上证明了TNAS的有效性和鲁棒性。</p>
<h3 id="38-水下图像增强-AutoEnhancer-Transformer-on-U-Net-Architecture-Search-for-Underwater-Image-Enhancement"><a href="#38-水下图像增强-AutoEnhancer-Transformer-on-U-Net-Architecture-Search-for-Underwater-Image-Enhancement" class="headerlink" title="38[水下图像增强]AutoEnhancer: Transformer on U-Net Architecture Search for Underwater Image Enhancement"></a>38[水下图像增强]<a class="link" target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ACCV2022/html/Tang_AutoEnhancer_Transformer_on_U-Net_Architecture_search_for_Underwater_Image_Enhancement_ACCV_2022_paper.html">AutoEnhancer: Transformer on U-Net Architecture Search for Underwater Image Enhancement <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>水下场景复杂多样，图像受到吸收、散射、颜色失真等因素的影响，Tang等人将基于U-Net的NAS方法应用到水下图像增强任务。该方法提出了一个新的搜索空间，包括常见的卷积、池化等操作，以及可选择的Transformer模块，可以自动选择合适的自注意力机制来提高特征表示能力。此外,该方法利用RGB和Lab两个颜色空间的图像作为网络输入来提升网络的泛化性。实验结果表明，该方法在三个水下图像数据集上均取得SOTA性能，并在低光照图像数据集上验证了可迁移性。</p>
<h3 id="39-训练-Nasvit-Neural-architecture-search-for-efficient-vision-transformers-with-gradient-conflict-aware-supernet-training"><a href="#39-训练-Nasvit-Neural-architecture-search-for-efficient-vision-transformers-with-gradient-conflict-aware-supernet-training" class="headerlink" title="39[训练]Nasvit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training"></a>39[训练]<a class="link" target="_blank" rel="noopener" href="https://openreview.net/forum?id=Qaw16njk6L">Nasvit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>超网和不同子网络的梯度之间存在的冲突问题会导致ViT的训练提前饱和,收敛性较差。Gong等人从三个方面是来改进训练方法：1）通过梯度投影方法来优先训练子网络而非超网；2）引入可切换的层缩放设计增加子网络的表达能力；3）减少数据增强和正则化，提供更容易的训练信号。通过改进的ViT超网络训练技术，发现了一系列优于之前所有的CNN和ViT的高效模型，称为NASViT，并在下游语义分割任务上展现出优异的迁移学习性能。</p>
<h3 id="40-基准-Nas-bench-nlp-neural-architecture-search-benchmark-for-natural-language-processing"><a href="#40-基准-Nas-bench-nlp-neural-architecture-search-benchmark-for-natural-language-processing" class="headerlink" title="40[基准]Nas-bench-nlp: neural architecture search benchmark for natural language processing"></a>40[基准]<a class="link" target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9762315/">Nas-bench-nlp: neural architecture search benchmark for natural language processing <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Klyuchnikov等人提出了第一个RNN派生的NAS基准数据集NAS-Bench-NLP。该基准包含各种RNN变体的搜索空间，该空间中训练了超过14k个架构，并对训练好的模型进行语义相关性和语言理解的评估。基准测试显示Bayesian Optimization(BO)和Tree-structured Parzen Estimator(TPE)算法取得了最佳性能。</p>
<h3 id="41-ViTAS-Vision-transformer-architecture-search"><a href="#41-ViTAS-Vision-transformer-architecture-search" class="headerlink" title="41[]ViTAS: Vision transformer architecture search"></a>41[]<a class="link" target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-19803-8_9">ViTAS: Vision transformer architecture search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>VIT直接应用CNN中的NAS方法进行搜索会造成训练不稳定，Su等人提出了循环权重共享机制来更加公平地训练超网络中的各个通道,还制定了identity shifting策略来搜索不同层次的操作类型，减少超网络评估中的多对一冗余问题。在ImageNet图像分类任务上,ViTAS搜索到的架构比DeiT和Twins等现有方法效果更好，迁移到COCO目标检测和ADE20K语义分割任务上也取得了SOTA结果。</p>
<h3 id="42-DARTFormer-Finding-The-Best-Type-Of-Attention"><a href="#42-DARTFormer-Finding-The-Best-Type-Of-Attention" class="headerlink" title="42[]DARTFormer: Finding The Best Type Of Attention"></a>42[]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/DARTFormer%3A-Finding-The-Best-Type-Of-Attention-Brown-Zhao/1859fb2b30a2e9d54cbb9605bdd6f270caac6d66">DARTFormer: Finding The Best Type Of Attention <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Brown等人提出了一个DARTS风格的NAS方法来有为给定任务找到最佳的Transformer注意力机制。具体来说，DARTFormer构建了一个包含多种注意力头的supernetwork,通过掩码验证准确率下降来评估每种注意力机制的重要性,然后选择对准确率影响最大的注意力机制。实验在多个NLP任务上表明,该方法可以找到最佳单一注意力机制，但直接混合使用不同注意力的效果不佳。</p>
<h3 id="43-语音识别-NAS-SCAE-Searching-Compact-Attention-based-Encoders-For-End-to-end-Automatic-Speech-Recognition"><a href="#43-语音识别-NAS-SCAE-Searching-Compact-Attention-based-Encoders-For-End-to-end-Automatic-Speech-Recognition" class="headerlink" title="43[语音识别]NAS-SCAE: Searching Compact Attention-based Encoders For End-to-end Automatic Speech Recognition"></a>43[语音识别]<em><a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/NAS-SCAE%3A-Searching-Compact-Attention-based-For-Liu-Li/b7f2789f44006fe1a6ffce98c117fb1bab9038b7">NAS-SCAE: Searching Compact Attention-based Encoders For End-to-end Automatic Speech Recognition <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></em></h3><p>Liu等人针对端到端语音识别任务提出了一种架构无关的NAS方法，NAS-SCAE包括一个融合了不同编码器（如Transformer，Conformer等）架构拓扑的搜索空间，一个可以在搜索过程中考虑编码器的性能和资源消耗的可微分搜索算法，并提出可调节的搜索方案来缓解可微分算法的联合优化问题。实验结果验证了NAS-SCAE设计紧凑编码器的有效性以及框架无关性。</p>
<h3 id="44-注意力-Neural-Architecture-Search-on-Efficient-Transformers-and-Beyond"><a href="#44-注意力-Neural-Architecture-Search-on-Efficient-Transformers-and-Beyond" class="headerlink" title="44[注意力]Neural Architecture Search on Efficient Transformers and Beyond"></a>44[注意力]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Neural-Architecture-Search-on-Efficient-and-Beyond-Liu-Li/86c8d930b492a4f9cadc6c60aecdaaded49acc86">Neural Architecture Search on Efficient Transformers and Beyond <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>针对标准Transformer中的Softmax注意力机制存在的计算复杂度高的问题,Liu等人提出同时搜索cosFormer架构(采用ReLU作为核函数并加入了cosine重新加权)和注意力类型的搜索空间,允许模型自动选择Softmax注意力或线性注意力，并以RankNAS作为NAS框架通过成对排序、搜索空间剪枝和硬件感知约束来加速搜索过程。在机器翻译和图像分类任务上的实验表明,搜索得到的混合注意力cosFormer架构计算效率更高。</p>
<h3 id="46-搜索空间-FlexiBERT-Are-Current-Transformer-Architectures-too-Homogeneous-and-Rigid"><a href="#46-搜索空间-FlexiBERT-Are-Current-Transformer-Architectures-too-Homogeneous-and-Rigid" class="headerlink" title="46[搜索空间]FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?"></a>46[搜索空间]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/FlexiBERT%3A-Are-Current-Transformer-Architectures-Tuli-Dedhia/1bcd42583a7b4475d3b456678e7f3752acd9edd1">FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid? <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>针对当前Transformer架构过于同质化和僵硬的问题,Tuli等人提出了一个异构灵活的Transformer模型族FlexiBERT。具体来说，在Transformer超参数空间中加入卷积和线性变换等操作以增加异质性，提出了新的投影层和相对位置编码使模型层之间的隐藏维度可变，并使用Transformer2vec学习每个模型的嵌入表示。此外,还提出了新的NAS策略，称为BOSHNAS，使用二阶梯度优化和异方差代理模型快速搜索最优模型。实验结果表明，BOSHNAS搜索得到的FlexiBERT-Mini相较于BERT-Mini参数更少但性能更好,FlexiBERT-Large超越当前SOTA模型的性能。</p>
<h3 id="47-文本到图像合成-Neural-Architecture-Search-With-a-Lightweight-Transformer-for-Text-to-Image-Synthesis"><a href="#47-文本到图像合成-Neural-Architecture-Search-With-a-Lightweight-Transformer-for-Text-to-Image-Synthesis" class="headerlink" title="47[文本到图像合成]Neural Architecture Search With a Lightweight Transformer for Text-to-Image Synthesis"></a>47[文本到图像合成]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Neural-Architecture-Search-With-a-Lightweight-for-Li-Wen/3469caf4b69978e4eee09ad1bcab64f016976f7d">Neural Architecture Search With a Lightweight Transformer for Text-to-Image Synthesis <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Li等人利用基于强化学习的NAS方法，以轻量级Transformer为搜索空间自动搜索出一个适用于文本到图像合成任务的生成器网络架构。该架构能够高效地融合文本和图像特征,生成高质量和高分辨率的图像。作者在CUB-200 Birds，Oxford-102 Flowers和COCO数据集上验证了方法的先进性。</p>
<h3 id="48-Lightspeech-Lightweight-and-fast-text-to-speech-with-neural-architecture-search"><a href="#48-Lightspeech-Lightweight-and-fast-text-to-speech-with-neural-architecture-search" class="headerlink" title="48[]Lightspeech: Lightweight and fast text to speech with neural architecture search"></a>48[]<a class="link" target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9414403/">Lightspeech: Lightweight and fast text to speech with neural architecture search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Luo等人将NAS应用于文本到语音合成模型，提出了LightSpeech，设计了一个轻量级运算(如多头自注意力、深度可分离卷积等)的搜索空间，并采用基于GBDT的NAS算法搜索。实验结果显示，与FastSpeech相比，搜索得到的轻量模型在保持音质的前提下,模型大小、计算复杂度和CPU推理速度均有改善。</p>
<h3 id="49-知识蒸馏-Few-shot-Task-agnostic-Neural-Architecture-Search-for-Distilling-Large-…"><a href="#49-知识蒸馏-Few-shot-Task-agnostic-Neural-Architecture-Search-for-Distilling-Large-…" class="headerlink" title="49[知识蒸馏]Few-shot Task-agnostic Neural Architecture Search for Distilling Large …"></a>49[知识蒸馏]<a class="link" target="_blank" rel="noopener" href="https://papers.nips.cc/paper_files/paper/2022/hash/b7c12689a89e98a61bcaa65285a41b7c-Abstract-Conference.html">Few-shot Task-agnostic Neural Architecture Search for Distilling Large … <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Xu等人利用NAS来自动地从预训练语言模型中搜索个性化的最优学生模型，称为AutoDistil，将Transformer搜索空间划分为多个子空间,每个子空间训练一个SuperLM，并利用自注意力关系迁移进行无任务标注的SuperLM训练，最后从训练好的SuperLM中直接提取出满足资源约束的最优学生模型，而无需额外训练或蒸馏。实验结果表明,与当前最优的知识蒸馏方法相比,AutoDistil可以获得3倍的计算量压缩,同时性能基本不变。</p>
<h3 id="50-one-shot-GraViT-E-Gradient-based-Vision-Transformer-Search-with-Entangled-Weights"><a href="#50-one-shot-GraViT-E-Gradient-based-Vision-Transformer-Search-with-Entangled-Weights" class="headerlink" title="50[one-shot]GraViT-E: Gradient-based Vision Transformer Search with Entangled Weights"></a>50[one-shot]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/GraViT-E%3A-Gradient-based-Vision-Transformer-Search-Sukthanker-Krishnakumar/452efa94e84c7b04e4f81387dbd7a797693f3ad7">GraViT-E: Gradient-based Vision Transformer Search with Entangled Weights <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>当前的one-shot架构搜索方法存在内存需求大和搜索偏差的问题，Sukthanker等人将weight entanglement和architecture参数的连续松弛相结合,既继承了weight entanglement对内存效率的优势,又可以应用gradient-based one-shot NAS方法进行搜索。在Vision Transformer的搜索空间上,作者分别与DARTS、DrNAS和GDAS这些one-shot优化器相结合,结果表明该方法在CIFAR和ImageNet数据集上优于AutoFormer基准,同时参数量也更加高效。</p>
<h3 id="51-SpeedLimit-Neural-Architecture-Search-for-Quantized-Transformer-Models"><a href="#51-SpeedLimit-Neural-Architecture-Search-for-Quantized-Transformer-Models" class="headerlink" title="51[]SpeedLimit: Neural Architecture Search for Quantized Transformer Models"></a>51[]<strong>SpeedLimit:</strong> <strong>Neural Architecture Search</strong> <strong>for Quantized Transformer Models</strong></h3><p>Chai等人提出了SpeedLimit，一种结合了两阶段NAS和8位整数（int8）量化的方法搜索最优的Transformer模型。具体来说，使用一个延迟预测器来缩小搜索空间，只保留满足延迟约束的int8量化模型的架构，然后训练一个超级模型（supermodel），使用进化搜索提取候选子模型（candidates）来进行微调、量化和评估。实验结果表明，SpeedLimit在准确性、延迟和内存占用方面都优于当前最先进的方法AutoTinyBERT。</p>
<h3 id="52-Efficient-gradient-based-neural-architecture-search-for-end-to-end-ASR"><a href="#52-Efficient-gradient-based-neural-architecture-search-for-end-to-end-ASR" class="headerlink" title="52[]Efficient gradient-based neural architecture search for end-to-end ASR"></a>52[]<a class="link" target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3461615.3491109">Efficient gradient-based neural architecture search for end-to-end ASR <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Shi等人将DARTS方法应用于端到端语音识别任务，称为Darts-Conformer，将Conformer的组成模块视为Darts节点，模块之间在一个有向无环图（DAG）中自由地连接和选择操作，形成搜索空间，并在小数据集上搜索最佳单元结构,再堆叠组成完整模型。该方法在两个数据集上均显著降低了字符错误率。</p>
<h3 id="53-AutoNLU-Architecture-Search-for-Sentence-and-Cross-sentence-Attention-Modeling-with-Re-designed-Search-Space"><a href="#53-AutoNLU-Architecture-Search-for-Sentence-and-Cross-sentence-Attention-Modeling-with-Re-designed-Search-Space" class="headerlink" title="53[]AutoNLU: Architecture Search for Sentence and Cross-sentence Attention Modeling with Re-designed Search Space"></a>53[]<a class="link" target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-88480-2_13">AutoNLU: Architecture Search for Sentence and Cross-sentence Attention Modeling with Re-designed Search Space <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Zhu等人通过NAS自动构建自然语言理解模型的编码器和跨句注意力机制，提出了AutoNLU,其设计了包含编码器操作、聚合器操作和将词嵌入、位置编码等相关设定的完备搜索空间，首次将跨句注意力建模作为NAS的目标,让控制器自动决定使用哪种注意力函数以及如何组合编码器，并通过跨操作及跨层参数共享等策略来改进搜索。结果表明,通过知识蒸馏学习到的模型优于基准,并且与BERT教师模型的性能差距较小。</p>
<h3 id="54-NASPY-A-UTOMATED-E-XTRACTION-OF-A-UTOMATED-M-ACHINE-L-EARNING-M-ODELS"><a href="#54-NASPY-A-UTOMATED-E-XTRACTION-OF-A-UTOMATED-M-ACHINE-L-EARNING-M-ODELS" class="headerlink" title="54[]NASPY : A UTOMATED E XTRACTION OF A UTOMATED M ACHINE L EARNING M ODELS"></a>54[]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/NASPY-%3A-A-UTOMATED-E-XTRACTION-OF-A-UTOMATED-M-L-M-Lou-Guo/c7201dec1a886679841822794a19add2798f0463">NASPY : A UTOMATED E XTRACTION OF A UTOMATED M ACHINE L EARNING M ODELS <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>现有的模型提取攻击不能有效地分析NAS引入的新型复杂操作和拓扑结构,Lou等人设计了NASPY，一个端到端的框架，利用缓存旁路和总线窥探等硬件攻击技术，从低级计算库（如OpenBLAS）的事件序列中恢复NAS模型的操作序列，然后通过序列分析和随机搜索等技术，从操作序列中提取操作的超参数和模型的拓扑结构。在此基础上还利用了序列到序列的深度学习模型（如RNN-CTC和Transformer）来提高操作序列识别的准确性和鲁棒性。实验结果表明,NASPY可以以很高的精度恢复NAS模型的完整架构。</p>
<h3 id="55-Searching-for-efficient-multi-stage-vision-transformers"><a href="#55-Searching-for-efficient-multi-stage-vision-transformers" class="headerlink" title="55[]Searching for efficient multi-stage vision transformers"></a>55[]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.00642">Searching for efficient multi-stage vision transformers <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Liao等人将空间降采样和NAS引入视觉变换器（ViT）中来提高效率和性能。具体来说，首先提出了残差空间降采样ViT-Res,将单阶段的ViT划分为多阶段,每阶段序列长度和embeddings大小相同,并在降低序列长度时添加跳跃连接,从而提高性能并稳定更深的网络训练。其次,利用ViT中的层归一化不依赖于批量维度的特性，提出了在前向/反向传播过程中采样多个不同子网络的方法ViT-ResNAS。在ImageNet上的实验表明,相比原始的DeiT,作者提出的ViT-ResNAS达到了更好的准确率-复杂度权衡。</p>
<h3 id="56-Searching-for-Efficient-Transformers-for-Language-Modeling"><a href="#56-Searching-for-Efficient-Transformers-for-Language-Modeling" class="headerlink" title="56[]Searching for Efficient Transformers for Language Modeling"></a>56[]<a class="link" target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2021/hash/2f3c6a4cd8af177f6456e7e51a916ff3-Abstract.html">Searching for Efficient Transformers for Language Modeling <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>So等人提出了一种基于TensorFlow程序的搜索空间，其中每个程序定义了一个可堆叠的解码器模块用于自回归语言建模，通过进化算法所搜索到的Primer模型包含两个简单而有效的修改：在前馈网络中使用平方ReLU激活函数，在自注意力中的多头投影后添加深度可分卷积层。实验结果表明,与原始Transformer相比,Primer能够在给定相同或更少的计算资源下达到更好的效果。</p>
<h3 id="57-病灶分割-T-AutoML-Automated-machine-learning-for-lesion-segmentation-using-transformers-in-3d-medical-imaging"><a href="#57-病灶分割-T-AutoML-Automated-machine-learning-for-lesion-segmentation-using-transformers-in-3d-medical-imaging" class="headerlink" title="57[病灶分割]T-AutoML: Automated machine learning for lesion segmentation using transformers in 3d medical imaging"></a>57[病灶分割]<a class="link" target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/html/Yang_T-AutoML_Automated_Machine_Learning_for_Lesion_Segmentation_Using_Transformers_in_ICCV_2021_paper.html?ref=https://githubhelp.com">T-AutoML: Automated machine learning for lesion segmentation using transformers in 3d medical imaging <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Yang等人将NAS方法用于医学图像中的病灶分割，T-AutoML设计了一个新的分割网络搜索空间，使得不同空间层次的特征图可以任意连接，并利用transformer模块处理可变长度的搜索空间嵌入，此外，提出了基于关系的预测器来预测不同训练配置之间的关系。实验在两个病灶分割数据集上（LiTS和MSD）验证了T-AutoML的有效性,并具有很好的可迁移性。</p>
<h3 id="58-Autosumm-Automatic-model-creation-for-text-summarization-https-aclanthology-org-2021-emnlp-main-798-utm-campaign-毎週-NLP-論文-amp-utm-medium-x3D-email-amp-utm-source-x3D-Revue-newsletter"><a href="#58-Autosumm-Automatic-model-creation-for-text-summarization-https-aclanthology-org-2021-emnlp-main-798-utm-campaign-毎週-NLP-論文-amp-utm-medium-x3D-email-amp-utm-source-x3D-Revue-newsletter" class="headerlink" title="58[][Autosumm: Automatic model creation for text summarization](https://aclanthology.org/2021.emnlp-main.798/?utm_campaign=毎週 NLP 論文&utm_medium=email&utm_source=Revue newsletter)"></a>58[][Autosumm: Automatic model creation for text summarization](<a class="link" target="_blank" rel="noopener" href="https://aclanthology.org/2021.emnlp-main.798/?utm_campaign=%E6%AF%8E%E9%80%B1">https://aclanthology.org/2021.emnlp-main.798/?utm_campaign=毎週 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> NLP 論文&amp;utm_medium=email&amp;utm_source=Revue newsletter)</h3><p>Nangi等人提出了一种自动创建文本摘要模型的框架，结合NAS和知识蒸馏技术，利用大规模预训练语言模型中的知识来进行模型搜索和压缩，并针对抽取式和生成式摘要任务分别提出了AUTOSUMM-CREATE和AUTOSUMM-DISTILL。在多个数据集上的实验展示了所生成的模型在推理时间和模型大小方面具有明显的优势。</p>
<h3 id="59-图像分类和密集预测任务-NASformer-Neural-architecture-search-for-vision-transformer"><a href="#59-图像分类和密集预测任务-NASformer-Neural-architecture-search-for-vision-transformer" class="headerlink" title="59[图像分类和密集预测任务]NASformer: Neural architecture search for vision transformer"></a>59[图像分类和密集预测任务]<a class="link" target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-02375-0_4">NASformer: Neural architecture search for vision transformer <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Vision Transformer存在全局自注意力机制计算复杂度高、网络架构设计原则不明确两个问题，Ni等人提出了扩张窗口自注意力机制,在保证全局特征表达能力的同时复杂度线性相关于图像大小,并基于此设计了NASformer的搜索空间,利用one-shot NAS搜索得到了最优的NASformer网络结构。在ImageNet图像分类、COCO目标检测和ADE20K语义分割任务上的实验结果表明,NASformer均优于当前最先进的CNN和Vision Transformer方法。</p>
<h3 id="60-Searching-the-search-space-of-vision-transformer"><a href="#60-Searching-the-search-space-of-vision-transformer" class="headerlink" title="60[]Searching the search space of vision transformer"></a>60[]<a class="link" target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2021/hash/48e95c45c8217961bf6cd7696d80d238-Abstract.html">Searching the search space of vision transformer <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>NAS的搜索空间设计依赖于人为的先验知识和固定的假设，Chen等人提出了动态变化VIT搜索空间。具体来说，首先定义了一个基于期望-最优误差（E-T Error）的搜索空间质量评估指标，利用one shot NAS方法比较不同搜索空间下子网的性能，接着采用逐步演化的策略，根据不同搜索维度的趋势动态地调整搜索空间的范围和粒度(如深度、窗口大小等)。实验结果表明，该方法在ImageNet图像分类上搜索得到的模型（S3）取得了最先进水平的性能，同时，在目标检测、语义分割和视觉问答任务，S3也展示了良好的泛化能力和迁移能力。</p>
<h3 id="62-搜索空间-密集预测-HR-NAS-Searching-Efficient-High-Resolution-Neural-Architectures-with-Lightweight-Transformers"><a href="#62-搜索空间-密集预测-HR-NAS-Searching-Efficient-High-Resolution-Neural-Architectures-with-Lightweight-Transformers" class="headerlink" title="62[搜索空间 密集预测]HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers"></a>62[搜索空间 密集预测]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/HR-NAS%3A-Searching-Efficient-High-Resolution-Neural-Ding-Lian/472fb54ea252d56bc28b2a5f7f4726fa9619e649">HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Ding等人将NAS方法应用于高分辨率图像的密集预测任务（如分割、检测、姿态估计等），HR-NAS设计了一个通过投影和解投影操作在降维空间进行自注意力计算的轻量级Transformer模块，构建了一个包含卷积和该Transformer模块的多分支搜索空间来同时建模多尺度和全局上下文信息，并通过逐渐剪枝卷积通道和Transformer查询来探索最优的特征组合。实验表明,HR-NAS在多个密集预测任务上都取得了SOTA性能。</p>
<h3 id="63-多模态-BM-NAS-Bilevel-Multimodal-Neural-Architecture-Search"><a href="#63-多模态-BM-NAS-Bilevel-Multimodal-Neural-Architecture-Search" class="headerlink" title="63[多模态]BM-NAS: Bilevel Multimodal Neural Architecture Search"></a>63[多模态]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/BM-NAS%3A-Bilevel-Multimodal-Neural-Architecture-Yin-Huang/0d0d3d8d5be06b418ca6d5d1eb73d2c96214376f">BM-NAS: Bilevel Multimodal Neural Architecture Search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Yin等人提出了用于多模态学习任务的NAS框架BM-NAS，在可微分NAS的基础上采用双层搜索策略,上层搜索多模态特征选择,下层搜索特征融合策略。实验在三个多模态数据集上(多标签电影类型分类、多模态动作识别和多模态手势识别)验证了该方法的有效性。</p>
<h3 id="64-架构编码-CATE-Computation-aware-Neural-Architecture-Encoding-with-Transformers"><a href="#64-架构编码-CATE-Computation-aware-Neural-Architecture-Encoding-with-Transformers" class="headerlink" title="64[架构编码]CATE: Computation-aware Neural Architecture Encoding with Transformers"></a>64[架构编码]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/CATE%3A-Computation-aware-Neural-Architecture-with-Yan-Song/8279cee52ae62fd819e1380b5e6ee200dab09aad">CATE: Computation-aware Neural Architecture Encoding with Transformers <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Yan等人提出了一种基于Transformer的计算感知神经架构编码方法CATE。CATE通过成对的预训练方式,使用交叉注意力的Transformer对计算相似的架构对进行编码,以masked language modeling(基于Floyd算法的遮挡机制)作为预训练目标，学习包含局部和全局计算信息的稠密上下文化的架构表示。实验结果表明,CATE对下游NAS任务尤其是在大规模搜索空间中是有益的,并展示了在训练搜索空间之外的泛化能力。</p>
<h3 id="65-多模态-MUFASA-Multimodal-Fusion-Architecture-Search-for-Electronic-Health-Records"><a href="#65-多模态-MUFASA-Multimodal-Fusion-Architecture-Search-for-Electronic-Health-Records" class="headerlink" title="65[多模态]MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records"></a>65[多模态]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/MUFASA%3A-Multimodal-Fusion-Architecture-Search-for-Xu-So/513a73c77b652a89a916673318d03caf801bea69">MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>电子健康记录(EHR)数据具有多模态性质，包括结构化数据(诊断代码、实验室结果等)和非结构化数据(临床文本)，不同模态的数据具有不同的表示和生成过程。Xu等人提出的MUFASA方法以Transformer作为基础架构，使用进化算法同时优化多模态特征融合策略（包括Early Fusion、Hybrid Fusion以及Late Fusion）和针对每个模态的架构。在MIMIC-III数据集上的实验结果表明,MUFASA找到的模型架构优于单模态NAS方法以及基线Transformer架构。</p>
<h3 id="66-架构-TNASP-A-Transformer-based-NAS-Predictor-with-a-Self-evolution-Framework"><a href="#66-架构-TNASP-A-Transformer-based-NAS-Predictor-with-a-Self-evolution-Framework" class="headerlink" title="66[架构]TNASP: A Transformer-based NAS Predictor with a Self-evolution Framework"></a>66[架构]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/TNASP%3A-A-Transformer-based-NAS-Predictor-with-a-Lu-Li/b699871a5eba6185e1ed32a2379aabf46016e699">TNASP: A Transformer-based NAS Predictor with a Self-evolution Framework <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>针对Predictor-based NAS方法存在空间拓扑信息表示不佳的问题，Lu等人提出了一种基于Transformer的多头自注意力机制来更好地编码离散架构图的特征，利用拉普拉斯矩阵的线性变换作为位置编码来进一步优化图结构数据的拓扑信息，并设计了一个自演化框架利用历史评估信息作为约束来指导预测器的训练。在NAS-Bench-101、NAS-Bench-201和DARTS搜索空间上的实验结果均超过了相关SOTA方法。</p>
<h3 id="67-高光谱图像分类-Spectral-Spatial-Transformer-Network-for-Hyperspectral-Image-Classification-A-Factorized-Architecture-Search-Framework"><a href="#67-高光谱图像分类-Spectral-Spatial-Transformer-Network-for-Hyperspectral-Image-Classification-A-Factorized-Architecture-Search-Framework" class="headerlink" title="67[高光谱图像分类]Spectral-Spatial Transformer Network for Hyperspectral Image Classification: A Factorized Architecture Search Framework"></a>67[高光谱图像分类]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Spectral-Spatial-Transformer-Network-for-Image-A-Zhong-Li/50fadb9e46139af8e84d9d50478430d1bbe4e081">Spectral-Spatial Transformer Network for Hyperspectral Image Classification: A Factorized Architecture Search Framework <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Zhong等人提出了一种新颖的光谱-空间Transformer网络（SSTN）和因子化的架构搜索框架，用于高光谱图像（HSI）分类。具体来说，首先提出了新的空间注意力模块和光谱关联模块,分别用于捕获长距离的特征交互和特征的稀疏表示，其次设计了因子化架构搜索框架,将NAS分解为层级操作选择和模块顺序选择两个子过程,避免了常规神经结构搜索中的双级优化困难。SSTN在五个流行的HSI数据集上都取得了优异的分类性能。</p>
<h3 id="68-知识蒸馏-Automatic-Student-Network-Search-for-Knowledge-Distillation"><a href="#68-知识蒸馏-Automatic-Student-Network-Search-for-Knowledge-Distillation" class="headerlink" title="68[知识蒸馏]Automatic Student Network Search for Knowledge Distillation"></a>68[知识蒸馏]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Automatic-Student-Network-Search-for-Knowledge-Zhang-Zhu/87f759eb8c2a8d718b5d1e27ea28bf73c15a922c">Automatic Student Network Search for Knowledge Distillation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Zhang等人提出基于NAS的知识蒸馏方法NAS-KD，通过自动搜索一个更紧凑和有效的学生网络来压缩BERT模型。NAS-KD包括两个模块：NAS模块和KD模块。在NAS模块中，作者设计了一个包含编码层（如LSTM，注意力，卷积等）和聚合层（如池化、动态路由等）的候选操作搜索空间，然后通过梯度下降算法得到一个学生单元。在KD模块中，作者将多个学生单元堆叠起来形成学生网络，并从教师网络BERT中蒸馏知识，并通过软目标损失，硬目标损失和中间层损失来度量师生网络的差异。最终的目标损失（三种损失函数的加权和）被反馈到NAS模块中用来更新学生单元的结构和参数。实验结果表明，与其他基于Transformer的BERT压缩方法相比,NAS-KD可以在性能损失很小的情况下显著减少模型的参数量。</p>
<h3 id="69-Glit-Neural-architecture-search-for-global-and-local-image-transformer"><a href="#69-Glit-Neural-architecture-search-for-global-and-local-image-transformer" class="headerlink" title="69[]Glit: Neural architecture search for global and local image transformer"></a>69[]<a class="link" target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/ICCV2021/html/Chen_GLiT_Neural_Architecture_Search_for_Global_and_Local_Image_Transformer_ICCV_2021_paper.html">Glit: Neural architecture search for global and local image transformer <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Chen等人使用首次使用NAS来寻找针对图像识别的Transformer架构的方法。为此，引入了基于一维卷积的局部模块来建模图像的局部相关性，并定义了可以自由权衡全局和局部信息的搜索空间，还设计了分层的NAS算法,先搜索模块的全局-局部分布,再搜索每个模块内部的细节架构。在ImageNet上的实验表明,作者的方法可以找到比ResNet和DeiT更优的Transformer模型。</p>
<h3 id="70-对比-x2F-集成-Bossnas-Exploring-hybrid-cnn-transformers-with-block-wisely-self-supervised-neural-architecture-search"><a href="#70-对比-x2F-集成-Bossnas-Exploring-hybrid-cnn-transformers-with-block-wisely-self-supervised-neural-architecture-search" class="headerlink" title="70[对比/集成]Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search"></a>70[对比/集成]<a class="link" target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/ICCV2021/html/Li_BossNAS_Exploring_Hybrid_CNN-Transformers_With_Block-Wisely_Self-Supervised_Neural_Architecture_Search_ICCV_2021_paper.html">Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>针对NAS存在的两个问题:(1)大量权重共享导致的架构排序不准确;(2)区块级监督导致的架构偏差，Li等人提出了BossNAS：在训练阶段,通过自监督对比学习的方式训练子网络,并提出Ensemble Bootstrapping的训练方式让子网络学习预测所有采样网络的集成概率。在搜索阶段,利用所有候选架构的集成概率作为公平的评估目标。作者还设计了一个新的混合CNN-Transformer搜索空间，叫做HyTra。作者在ImageNet等数据集上证明了BossNAS方法的有效性和泛化能力。</p>
<h3 id="71-Autoformer-Searching-transformers-for-visual-recognition"><a href="#71-Autoformer-Searching-transformers-for-visual-recognition" class="headerlink" title="71[]Autoformer: Searching transformers for visual recognition"></a>71[]<a class="link" target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/ICCV2021/html/Chen_AutoFormer_Searching_Transformers_for_Visual_Recognition_ICCV_2021_paper.html">Autoformer: Searching transformers for visual recognition <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Chen等人首次将NAS用于搜索Vision Transformer网络架构，Autoformer提出了weight entanglement的supernet训练策略，其思想是让同一层中不同的Transformer块尽可能地共享权重，使得从训练好的supernet中采样出的子网可以不需要额外的fine-tuning就能达到从头训练的效果。论文在ImageNet数据集上验证了方法,结果显示AutoFormer家族的模型相比手工设计的SOTA Vision Transformer模型效果更好。</p>
<h3 id="72-NAS-BERT-task-agnostic-and-adaptive-size-BERT-compression-with-neural-architecture-search"><a href="#72-NAS-BERT-task-agnostic-and-adaptive-size-BERT-compression-with-neural-architecture-search" class="headerlink" title="72[]NAS-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search"></a>72[]<a class="link" target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3447548.3467262">NAS-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Xu等人使用NAS在预训练任务上对BERT进行模型压缩，可以直接应用到不同下游任务，避免每个任务重新压缩，并使用分块搜索、搜索空间（包括多头注意力、可分离卷积等）剪枝和性能逼近等技术提高搜索效率。实验结果表明,NAS-BERT可以搜索出更好的轻量级BERT模型,并可以直接应用到下游GLUE和SQuAD任务中。</p>
<h3 id="73-架构优化-Towards-Accurate-and-Compact-Architectures-via-Neural-Architecture-Transformer-Transformer应用于-NAS"><a href="#73-架构优化-Towards-Accurate-and-Compact-Architectures-via-Neural-Architecture-Transformer-Transformer应用于-NAS" class="headerlink" title="73[架构优化]Towards Accurate and Compact Architectures via Neural Architecture Transformer(Transformer应用于*NAS)*"></a>73[架构优化]<a class="link" target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Towards-Accurate-and-Compact-Architectures-via-Guo-Zheng/46ca6bcf79895b6865527fefa516674be5ade953">Towards Accurate and Compact Architectures via Neural Architecture Transformer <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><em>(Transformer应用于*<em>NAS</em></em>)*</h3><p>Guo等人提出了一种神经网络架构转换器（NAT），通过替换其中低效的操作（比如卷积或池化）为更高效的操作（比如跳跃连接或空连接），来输出一个优化后的架构。作者将这个问题建模为一个马尔可夫决策过程（MDP），并利用图卷积网络（图可以表示并处理架构之间的邻接关系信息）作为控制器来学习替换冗余模块的策略。作者在多个基准数据集上进行了实验，NAT/NAT++(更大的搜索空间以及更细粒度的架构优化)可以有效地优化各种架构。</p>
<h3 id="74-Autotrans-Automating-transformer-design-via-reinforced-architecture-search"><a href="#74-Autotrans-Automating-transformer-design-via-reinforced-architecture-search" class="headerlink" title="74[]Autotrans: Automating transformer design via reinforced architecture search"></a>74[]<a class="link" target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-88480-2_14">Autotrans: Automating transformer design via reinforced architecture search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Zhu等人通过NAS算法来指导transformer架构设计，设计了一个包含Transformer超参数的搜索空间，并通过跨操作参数共享策略和部分训练数据搜索来加速和正则化。实验在 CoNLL03、Multi30k 和 WMT14 数据集上表明,搜索得到的 transformer 架构，且学习率变化更加稳定。</p>
<h3 id="75-PLM-AutoTinyBERT-Automatic-Hyper-parameter-Optimization-for-Efficient-Pre-…"><a href="#75-PLM-AutoTinyBERT-Automatic-Hyper-parameter-Optimization-for-Efficient-Pre-…" class="headerlink" title="75[PLM]AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre …"></a>75[PLM]<a class="link" target="_blank" rel="noopener" href="https://aclanthology.org/2021.acl-long.400/">AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre … <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Yin等人提出AutoTinyBERT来为高效的预训练语言模型设计合适的架构超参数，而非沿用BERT的默认值。其训练了一个包含所有子架构的SuperPLM作为代理，在评估时只需要提取对应的子模型，而无需从头训练。在此基础上,限制了搜索空间为层结构相同的模型，并使用多个子batch训练和fine-tuning权重继承的手段提高进化算法的搜索效率。在GLUE和SQuAD两个基准上，AutoTinyBERT均优于预训练和知识蒸馏两种训练方式下BERT及其变体的超参数设置。</p>
<h3 id="76-知识蒸馏-Accelerating-Neural-Architecture-Search-for-Natural-Language-Processing-with-Knowledge-Distillation-and-Earth-Mover’s-Distance"><a href="#76-知识蒸馏-Accelerating-Neural-Architecture-Search-for-Natural-Language-Processing-with-Knowledge-Distillation-and-Earth-Mover’s-Distance" class="headerlink" title="76[知识蒸馏]Accelerating Neural Architecture Search for Natural Language Processing with Knowledge Distillation and Earth Mover’s Distance"></a>76[知识蒸馏]<a class="link" target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3404835.3463017">Accelerating Neural Architecture Search for Natural Language Processing with Knowledge Distillation and Earth Mover’s Distance <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Li等人认为NAS应用利用预训练好的教师模型来指导学生模型的的架构搜索，而不是在验证集上搜索最优架构。由于教师模型和学生模型有不同类型的隐藏层，作者使用地球移动距离(EMD)来度量距离，学习一个多对多的层映射关系，来最小化教师模型（BERT）和搜索网络之间的知识损失。此外，还在每一层连接隐藏节点后添加addnorm进行归一化来缓解梯度消失。实验结果表明,与DARTS和SNAS等方法相比,该方法在多个NLP数据集上取得了更好的性能,同时显著缩短了搜索时间。</p>
<h3 id="77-RankNAS-Efficient-Neural-Architecture-Search-by-Pairwise-Ranking"><a href="#77-RankNAS-Efficient-Neural-Architecture-Search-by-Pairwise-Ranking" class="headerlink" title="77[]RankNAS: Efficient Neural Architecture Search by Pairwise Ranking"></a>77[]<a class="link" target="_blank" rel="noopener" href="https://www.google.com/url?client=internal-element-cse&cx=000299513257099441687:fkkgoogvtaw&q=https://aclanthology.org/2021.emnlp-main.191.pdf&sa=U&ved=2ahUKEwjz7aKvqLiAAxVkpVYBHYdWDdUQFnoECAYQAg&usg=AOvVaw3ms-FYYoDquwO70QquLW_U">RankNAS: Efficient Neural Architecture Search by Pairwise Ranking <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>NAS评估每个架构性能需要训练大量样本到收敛，Hu等人将NAS表述为一个排序任务，而不需要预测准确的性能。为了高效解决排序问题,作者使用GBDT构建评分函数将其转换为一个二分类任务，只需要区分“好”和“坏”的候选架构。此外,作者还提出了一种搜索空间修剪方法,可以忽略不重要的架构特征从而进一步提升效率。在机器翻译和语言建模任务上的实验表明,RankNAS可以以远低于传统NAS方法的计算成本搜索出性能更优的模型架构。</p>
<h3 id="78-Memory-Efficient-Differentiable-Transformer-Architecture-Search"><a href="#78-Memory-Efficient-Differentiable-Transformer-Architecture-Search" class="headerlink" title="78[]Memory-Efficient Differentiable Transformer Architecture Search"></a>78[]<a class="link" target="_blank" rel="noopener" href="https://aclanthology.org/2021.findings-acl.372/">Memory-Efficient Differentiable Transformer Architecture Search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>针对DARTS在Transformer上的应用存在显存占用大的问题,Zhao等人提出了一种多分支可逆网络结构。该网络结构可以在反向传播时重构中间层的输出,因此只需要存储最后一层的输出,作者结合该可逆网络和DARTS,极大减少了显存占用。在机器翻译任务的实验中,DARTSformer比标准Transformer性能更好,同时可以泛化到多个数据集上。</p>
<h3 id="79-Neural-architecture-search-algorithm-to-optimize-deep-Transformer-model-for-fault-detection-in-electrical-power-distribution-systems"><a href="#79-Neural-architecture-search-algorithm-to-optimize-deep-Transformer-model-for-fault-detection-in-electrical-power-distribution-systems" class="headerlink" title="79[]Neural architecture search algorithm to optimize deep Transformer model for fault detection in electrical power distribution systems"></a>79[]Neural architecture search algorithm to optimize deep Transformer model for fault detection in electrical power distribution systems</h3><p>Thomas等人将DARTS算法用于搜索最优的Transformer模型，对架构参数和网络权重进行可微分的联合优化，并应用于检测和定位电力分配系统中的不同故障和不确定条件。作者在标准的IEEE 14母线配电系统和VSB输电线故障检测数据库上进行了故障分析来验证方法的有效性。</p>
<h3 id="81-HCT-net-hybrid-CNN-transformer-model-based-on-a-neural-architecture-search-network-for-medical-image-segmentation"><a href="#81-HCT-net-hybrid-CNN-transformer-model-based-on-a-neural-architecture-search-network-for-medical-image-segmentation" class="headerlink" title="81[]HCT-net: hybrid CNN-transformer model based on a neural architecture search network for medical image segmentation"></a>81[]HCT-net: hybrid CNN-transformer model based on a neural architecture search network for medical image segmentation</h3><p>Yu等人提出混合U型CNN和键采样transformer来分别提取特征的局部和全局信息，用于医学图像分割。此外，作者使用一种单路径NAS策略，利用一维编码分布来快速地在每条边上找到合适的操作，并同时搜索三种类型单元（Extraction Cell、Extension Cell、Equalization Cell）所组合的最优U型CNN。在多个医学图像数据集上的实验结果证明了该方法的有效性和泛化性。</p>
<h3 id="82-AutoTaskFormer-Searching-Vision-Transformers-for-Multi-task-Learning"><a href="#82-AutoTaskFormer-Searching-Vision-Transformers-for-Multi-task-Learning" class="headerlink" title="82[]AutoTaskFormer: Searching Vision Transformers for Multi-task Learning"></a>82[]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.08756">AutoTaskFormer: Searching Vision Transformers for Multi-task Learning <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p> Liu等人提出了一个新的端到端的神经架构搜索框架AutoTaskFormer用于自动设计多任务VIT，关键在于提出了骨架空间和单元空间,前者用于搜索每个任务的网络结构,后者用于搜索每个层的详细参数，并设计了新的多任务NAS流程，在训练的同时进行骨架空间搜索,利用Gumbel-softmax技巧优化每个任务的骨架分布，然后利用进化算法进行单元空间搜索来搜索多任务子网。实验结果显示,在小数据集(2任务Cityscape和3任务NYUv2)和大数据集(16任务Taskonomy)上,AutoTaskFormer均取得SOTA性能。</p>
<h3 id="83-硬件-Hyperscale-Hardware-Optimized-Neural-Architecture-Search"><a href="#83-硬件-Hyperscale-Hardware-Optimized-Neural-Architecture-Search" class="headerlink" title="83[硬件]Hyperscale Hardware Optimized Neural Architecture Search"></a>83[硬件]<a class="link" target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3582016.3582049">Hyperscale Hardware Optimized Neural Architecture Search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Li等人提出了H2O-NAS, 一个面向超大规模硬件异构数据中心的NAS方法。首先设计了一种大规模并行的单步RL搜索算法,可以在线从真实流量中同时学习架构策略和模型权重，并针对VIT、CNN等模型设计了硬件优化的搜索空间，最后提出了单边ReLU多目标奖励函数来平衡多个约束，以及一个“仿真-硬件微调”两阶段的性能预测模型。论文不仅证明了H2O-NAS在超大规模硬件上进行NAS的有效性，还开源了CoAtNet-H和EfficientNet-H两个新的模型族。</p>
<h3 id="84-基准-Neural-architecture-search-as-multiobjective-optimization-benchmarks-Problem-formulation-and-performance-assessment"><a href="#84-基准-Neural-architecture-search-as-multiobjective-optimization-benchmarks-Problem-formulation-and-performance-assessment" class="headerlink" title="84[基准]Neural architecture search as multiobjective optimization benchmarks: Problem formulation and performance assessment"></a>84[基准]<a class="link" target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10004638/">Neural architecture search as multiobjective optimization benchmarks: Problem formulation and performance assessment <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Lu等人提出了EvoXBench,一个可以无需GPU或PyTorch/TensorFlow等深度学习框架即可运行的NAS基准测试平台，方便评估进化多目标优化算法下的NAS性能。EvoXBench包含七种搜索空间，涵盖卷积神经网络和VIT两种类型的架构，以及CIFAR-10和ImageNet两种数据集。最后,作者利用6种代表性的进化多目标优化算法在该测试套件上进行了实验,验证了基准测试的有效性。</p>
<h3 id="85-知识蒸馏-Neural-Architecture-Search-for-Effective-Teacher-Student-Knowledge-Transfer-in-Language-Models"><a href="#85-知识蒸馏-Neural-Architecture-Search-for-Effective-Teacher-Student-Knowledge-Transfer-in-Language-Models" class="headerlink" title="85[知识蒸馏]Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models"></a>85[知识蒸馏]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.09639">Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Trivedi等人提出了一种基于知识蒸馏的NAS方法KD-NAS,旨在找到给定老师模型（如BERT）和下游任务的最佳学生模型架构。论文将知识蒸馏目标与传统的精度和延迟指标相结合作为NAS的奖励函数，使用LSTM控制器预测每个候选架构的奖励，并利用之前探索过的高性能架构的信息来指导搜索过程。在多语种自然语言推理任务上的实验结果表明,该方法自动找到了适合从BERT老师模型中蒸馏知识的最优学生模型架构。</p>
<h3 id="87-语义分割-Towards-real-time-segmentation-on-the-edge"><a href="#87-语义分割-Towards-real-time-segmentation-on-the-edge" class="headerlink" title="87[语义分割]Towards real-time segmentation on the edge"></a>87[语义分割]<a class="link" target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/25232">Towards real-time segmentation on the edge <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>针对边缘设备上的实时语义分割任务，Li等人设计了一种混合模块,结合inverted residual block和自注意力机制用于提取局部细节特征和全局空间依赖关系（通过Gumbel softmax采样选择）。此外,论文提出了新的搜索范式，从双分支超网开始，通过通道剪枝和基于梯度的搜索逐步优化块的宽度和选择，并利用辅助损失、自我蒸馏和MLP的延迟预测模型提升搜索性能。在Cityscapes等数据集上的实验结果证明了该方法在移动端实时语义分割效率与精度的先进性。</p>
<h3 id="88-LLMatic-Neural-Architecture-Search-via-Large-Language-Models-and-Quality-Diversity-Optimization"><a href="#88-LLMatic-Neural-Architecture-Search-via-Large-Language-Models-and-Quality-Diversity-Optimization" class="headerlink" title="88[]LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization"></a>88[]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.01102">LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Nasir等人提出LLMatic算法，利用大语言模型存储的知识来指导NAS，其将LLM视为变异和交叉操作符，生成新的神经网络架构，并使用质量多样性算法MAP-Elites维护两个归档，一个存储神经网络架构，另一个存储生成代码的提示和温度参数。实验结果表明，LLMatic可以在只使用2000次搜索和6.1亿参数的LLM的情况下找到有竞争力的神经网络架构。</p>
<h3 id="89-https-arxiv-org-pdf-2206-01191-pdf"><a href="#89-https-arxiv-org-pdf-2206-01191-pdf" class="headerlink" title="89[]https://arxiv.org/pdf/2206.01191.pdf"></a>89[]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.01191.pdf">https://arxiv.org/pdf/2206.01191.pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Li等人提出在可以移动设备上实现实时高精度的图像识别的EfficientFormer（基于vision transformer），其遵循维度一致的设计原则,将网络分为基于4D张量的卷积块和基于3D张量的自注意力块,避免了维度不匹配导致的张量切换，并提出基于延迟驱动的网络剪枝搜索方法，在预定义的超网中搜索到“精度损失/延迟降低”比值最小的剪枝操作。在图像分类、目标检测和语义分割等任务的实验结果表明，EfficientFormer在不同的硬件和编译器上都能达到超快的推理速度，同时保持高水平的性能。</p>
<h3 id="90-https-arxiv-org-pdf-2212-08059-pdf"><a href="#90-https-arxiv-org-pdf-2212-08059-pdf" class="headerlink" title="90[]https://arxiv.org/pdf/2212.08059.pdf"></a>90[]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.08059.pdf">https://arxiv.org/pdf/2212.08059.pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Li等人考虑同时优化模型的参数量和推理速度等多个指标，提出了EfficientFormerV2模型。其使用一个可调节深度和宽度的超网络，并通过评估每个缩减操作对准确率和移动效率分数（MES）的影响来找到最佳的子网络。作者在ImageNet-1K分类任务上验证了EfficientFormerV2模型系列的有效性（EfficientFormerV2-S2准确率相较于EfficientFormer-L1提升2.4%）。</p>
<h3 id="91-Multi-dimensional-vision-transformer-compression-via-dependency-guided-gaussian-process-search"><a href="#91-Multi-dimensional-vision-transformer-compression-via-dependency-guided-gaussian-process-search" class="headerlink" title="91[]Multi-dimensional vision transformer compression via dependency guided gaussian process search"></a>91[]<a class="link" target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2022W/EVW/html/Hou_Multi-Dimensional_Vision_Transformer_Compression_via_Dependency_Guided_Gaussian_Process_Search_CVPRW_2022_paper.html">Multi-dimensional vision transformer compression via dependency guided gaussian process search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Hou等人提出了一个多维视觉transformer压缩框架,通过联合剪枝attention head, neuron和sequence维度来压缩预训练的transformer模型。论文首先用Hilbert-Schmidt交叉协方差算子的范数来衡量不同维度特征与模型输出预测之间的统计依赖关系,以识别无用的组件。在此基础上，将多维压缩建模为一个优化问题,以权重共享的训练机制计算预算约束下学习不同维度的最优剪枝比率（类似于NAS中的架构），并采用高斯过程建模准确率，以期望改进量作为指标进行搜索（类似于NAS中的代理）。在ImageNet数据集上压缩DeiT和T2T-ViT模型,论文方法实现了40%<del>60%的FLOPs减少,带来1.3</del>2.2倍的实际加速,同时准确率无明显下降。</p>
<h3 id="92-Sparse-structure-search-for-delta-tuning"><a href="#92-Sparse-structure-search-for-delta-tuning" class="headerlink" title="92[]Sparse structure search for delta tuning"></a>92[]<a class="link" target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/4027fc4573ec9114182d1fef37a6321a-Abstract-Conference.html">Sparse structure search for delta tuning <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Delta调优只训练预训练语言模型的一小部分参数（绝大部分参数固定），是一类参数高效的调优技术，Hu等人将NAS算法引入Delta调优，称为S3Delta方法，将不同类型的可微调模块(如适配器等)以概率门控的方式嵌入到PTM内部各层的不同位置,构建一个统一的搜索空间。然后利用DARTS在这个空间内寻找最优的可微调模块布局结构,同时使用shifted global sigmoid方法显式控制参与微调的可训练参数数量。实验结果表明，S3Delta能够在GLUE和SuperGLUE两个多任务基准上，使用极少的可训练参数（0.01%∼0.35%）达到接近或超过全微调的性能。</p>
<h3 id="93-推荐-Towards-automatic-discovering-of-deep-hybrid-network-architecture-for-sequential-recommendation"><a href="#93-推荐-Towards-automatic-discovering-of-deep-hybrid-network-architecture-for-sequential-recommendation" class="headerlink" title="93[推荐]Towards automatic discovering of deep hybrid network architecture for sequential recommendation"></a>93[推荐]<a class="link" target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3485447.3512066">Towards automatic discovering of deep hybrid network architecture for sequential recommendation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Cheng等人针对顺序推荐（SR）问题提出了NASR，通过表状的搜索空间来自动地选择深度混合模型中每一层的操作（如卷积、自注意力），并采用权重共享、块级搜索、自监督训练等技术来降低搜索成本。实验结果表明，NASR找到的深度混合SR模型可以显著提高推荐性能。</p>
<h3 id="94-Neural-prompt-search"><a href="#94-Neural-prompt-search" class="headerlink" title="94[]Neural prompt search"></a>94[]<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.04673">Neural prompt search <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>针对最近兴起的参数高效调优方法(prompt module),比如Adapter、LoRA和VPT,在视觉任务上的表现不稳定的问题,Zhang等人提出了一个名为NOAH的NAS方法来自动搜索最优的prompt module设计。论文将现有的prompt module融入搜索空间,并使用了 AutoFormer这种基于权重共享的一次性搜索算法探索针对每个下游任务最优的prompt module组合。本文在超过 20 个视觉数据集上证明了 NOAH 的优越性。</p>
<h3 id="95-图-HGNAS-efficient-architecture-search-for-heterogeneous-graph-neural-networks"><a href="#95-图-HGNAS-efficient-architecture-search-for-heterogeneous-graph-neural-networks" class="headerlink" title="95[图]HGNAS++: efficient architecture search for heterogeneous graph neural networks"></a>95[图]<a class="link" target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10040227/">HGNAS++: efficient architecture search for heterogeneous graph neural networks <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><p>Gao等人提出了一个基于强化学习的异构图神经网络架构搜索方法HGNAS,以及改进版本HGNAS++。定义了一个通用的HGNNs框架，在此基础上设计了一个包含编码函数、聚合函数和混合聚合函数的搜索空间，并提出使用强化学习中的策略梯度方法训练RNN控制器来进行采样。此外,HGNAS++通过构建生成对抗网络框架,引入一个鉴别器来指导控制器逼近最优HGNN架构分布。实验结果表明,HGNAS可以设计出优于人工设计的HGNN模型的新型HGNN架构，HGNAS++可以明显减少候选HGNN架构的评估次数,降低算法搜索时间。</p>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> NAS-LLM综述</li>
        <li><strong>Author:</strong> Shuai Lv</li>
        <li><strong>Created at:</strong> 2023-07-19 20:19:44</li>
        
            <li>
                <strong>Updated at:</strong> 2023-08-22 21:15:35
            </li>
        
        <li>
            <strong>Link:</strong> https://blogls.top/2023/07/19/NAS-LLM综述/
        </li>
        <li>
            <strong>License:</strong> This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a>.
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/LLM/">#LLM</a>&nbsp;
                        </li>
                    
                </ul>
            

            <div class="recommended-article">
  <div class="recommended-article-header">
    <i aria-hidden="true"></i><span>推荐阅读</span>
  </div>
  <div class="recommended-article-group"><a class="recommended-article-item" href="/2023/06/27/Efficient-LLM/" title="Efficient LLM" rel="bookmark">
  <img src="/images/wallhaven-wqery6-light.webp" alt="Efficient LLM">
  <span class="title">Efficient LLM</span>
</a><a class="recommended-article-item" href="/2023/07/18/2023AAAI-A-Survey-on-Model-Compression-and-Acceleration-for-Pretrained-Language-Models/" title="2023AAAI-A Survey on Model Compression and Acceleration for Pretrained Language Models" rel="bookmark">
  <img src="/images/wallhaven-wqery6-light.webp" alt="2023AAAI-A Survey on Model Compression and Acceleration for Pretrained Language Models">
  <span class="title">2023AAAI-A Survey on Model Compression and Acceleration for Pretrained Language Models</span>
</a><a class="recommended-article-item" href="/2023/06/20/Efficient-AI-Google研究院综述-2023/" title="Efficient AI-Google研究院综述-2023" rel="bookmark">
  <img src="/images/wallhaven-wqery6-light.webp" alt="Efficient AI-Google研究院综述-2023">
  <span class="title">Efficient AI-Google研究院综述-2023</span>
</a></div>
</div>

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2023/07/19/LLM-LLaMA2/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">LLM-LLaMA2</span>
                                    <span class="post-nav-item">Prev posts</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2023/07/19/%E7%BB%8F%E5%85%B8%E7%9A%84%E8%BF%9E%E7%BB%AD%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">经典的连续学习算法</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;Comments
    </div>
    

        
            
    <div id="gitalk-container"></div>
    <script data-pjax
            src="//cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js"></script>
    <script data-pjax>

        function loadGitalk() {
            let __gitalk__pathname = decodeURI(location.pathname);
            const __gitalk__pathnameLength = __gitalk__pathname.length;
            const __gitalk__pathnameMaxLength = 50;
            if (__gitalk__pathnameLength > __gitalk__pathnameMaxLength) {
                __gitalk__pathname = __gitalk__pathname.substring(0, __gitalk__pathnameMaxLength - 3) + '...';
            }

            try {
                Gitalk && new Gitalk({
                    clientID: 'ca3b187a2eb63dfd2d20',
                    clientSecret: 'e96896908ba4dc10bb61a89de608ea3a0ce32013',
                    repo: 'Gitalk',
                    owner: 'ShuaiLv-JNU',
                    admin: ['ShuaiLv-JNU'],
                    id: __gitalk__pathname,
                    language: 'en'
                }).render('gitalk-container');

            } catch (e) {
                window.Gitalk = null;
            }
        }

        if ('true') {
            const loadGitalkTimeout = setTimeout(() => {
                loadGitalk();
                clearTimeout(loadGitalkTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadGitalk);
        }
    </script>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">NAS-LLM综述</div>
        <ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%AE%A1%E7%AE%97%E3%80%81%E6%B3%9B%E5%8C%96%E9%97%AE%E9%A2%98-%E5%89%AA%E6%9E%9D%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4-Ddpnas-Efficient-neural-architecture-search-via-dynamic-distribution-pruning"><span class="nav-text">1[计算、泛化问题-剪枝搜索空间]Ddpnas: Efficient neural architecture search via dynamic distribution pruning </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%B3%9B%E5%8C%96%E9%97%AE%E9%A2%98-NAS-MoE%E8%87%AA%E9%80%82%E5%BA%94-AutoMoE-Heterogeneous-Mixture-of-Experts-with-Adaptive-Computation-for-Efficient-Neural-Machine-Translation"><span class="nav-text">2[泛化问题-NAS+MoE自适应]AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%BC%A5%E5%90%88%E4%B8%8A%E4%B8%8B%E6%B8%B8%E5%B7%AE%E8%B7%9D-%E7%BB%9F%E4%B8%80%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%92%8C%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-GAIA-Universe-Everything-is-Super-Netify-IEEE-Xplore"><span class="nav-text">3[弥合上下游差距-统一目标检测和语义分割]GAIA-Universe: Everything is Super-Netify - IEEE Xplore </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E8%AF%84%E4%BC%B0%E3%80%81%E5%BC%A5%E5%90%88%E8%B6%85%E5%AD%90%E7%BD%91%E5%B7%AE%E8%B7%9D%E9%97%AE%E9%A2%98-MoE%E6%8E%A7%E5%88%B6%E5%AD%90%E7%BD%91%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB%E7%A8%8B%E5%BA%A6-Mixture-of-Supernets-Improving-Weight-Sharing-Supernet-Training-with-Architecture-Routed-Mixture-of-Experts"><span class="nav-text">4[评估、弥合超子网差距问题-MoE控制子网权重共享程度]Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E9%AB%98%E6%95%88%E7%9A%84PLM%E5%BE%AE%E8%B0%83-%E5%89%AA%E6%9E%9D%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4%EF%BC%9F-Neural-Architecture-Search-for-Parameter-Efficient-Fine-tuning-of-Large-Pre-trained-Language-Models"><span class="nav-text">5[高效的PLM微调-剪枝搜索空间？]Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98-NAS-%E5%9B%BE-Autogt-Automated-graph-transformer-architecture-search"><span class="nav-text">6[性能问题-NAS+图]Autogt: Automated graph transformer architecture search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E6%93%8D%E4%BD%9C%E5%92%8C%E6%9E%B6%E6%9E%84%E7%9A%84%E8%A1%A8%E7%A4%BA%E8%83%BD%E5%8A%9B-NAR-Former-Neural-Architecture-Representation-Learning-towards-Holistic-Attributes-Prediction"><span class="nav-text">7[操作和架构的表示能力]NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-%E7%9C%BC%E5%8A%A8%E4%BC%B0%E8%AE%A1%E9%97%AE%E9%A2%98-HRNAS%E7%9A%84%E6%94%B9%E8%BF%9B-Searching-Efficient-Neural-Architecture-with-Multi-resolution-Fusion-Transformer-for-Appearance-based-Gaze-Estimation"><span class="nav-text">8[眼动估计问题-HRNAS的改进]Searching Efficient Neural Architecture with Multi-resolution Fusion Transformer for Appearance-based Gaze Estimation </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-NAS%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4%E5%8A%A0%E5%85%A5%E5%85%88%E9%AA%8C-GPT-NAS-Neural-Architecture-Search-with-the-Generative-Pre-Trained-Model"><span class="nav-text">9[NAS搜索空间加入先验]GPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-%E4%BD%8E%E8%B5%84%E6%BA%90-%E6%B7%B7%E5%90%88%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4-HyT-NAS-Hybrid-Transformers-Neural-Architecture-Search-for-Edge-Devices"><span class="nav-text">10[低资源-混合搜索空间]HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-%E7%BB%9F%E4%B8%80%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1-%E5%A4%9A%E4%BB%BB%E5%8A%A1-MDL-NAS-A-Joint-Multi-Domain-Learning-Framework-for-Vision-Transformer"><span class="nav-text">11[统一下游任务-多任务]MDL-NAS: A Joint Multi-Domain Learning Framework for Vision Transformer </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-NAS%E6%8E%92%E5%90%8D-PerfHD-Efficient-ViT-Architecture-Performance-Ranking-%E2%80%A6"><span class="nav-text">12[NAS排名]PerfHD: Efficient ViT Architecture Performance Ranking … </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-%E6%B7%B7%E5%90%88%E6%9E%B6%E6%9E%84-%E6%97%A0%E9%9C%80%E8%AE%AD%E7%BB%83-%E8%AE%BE%E8%AE%A1%E6%8C%87%E6%A0%87-AutoST-Training-free-Neural-Architecture-Search-for-Spiking-Transformers"><span class="nav-text">13[混合架构-无需训练-设计指标]AutoST: Training-free Neural Architecture Search for Spiking Transformers </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-%E6%9E%B6%E6%9E%84%E8%A1%A8%E7%A4%BA%E8%83%BD%E5%8A%9B-PINAT-A-Permutation-INvariance-Augmented-Transformer-for-NAS-Predictor-Transformer%E7%94%A8%E4%BA%8ENAS"><span class="nav-text">14[架构表示能力]PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor (Transformer用于NAS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-%E6%97%A0%E9%9C%80%E8%AE%AD%E7%BB%83-%E8%AE%BE%E8%AE%A1%E6%8C%87%E6%A0%87-Training-free-Neural-Architecture-Search-for-RNNs-and-Transformers"><span class="nav-text">15[无需训练-设计指标]Training-free Neural Architecture Search for RNNs and Transformers </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-%E7%BC%A9%E5%B0%8F%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4-PreNAS-Preferred-One-Shot-Learning-Towards-Efficient-Neural-Architecture-Search"><span class="nav-text">16[缩小搜索空间]PreNAS: Preferred One-Shot Learning Towards Efficient Neural Architecture Search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-%E8%B6%85%E7%BD%91%E4%B8%8E%E5%AD%90%E7%BD%91%E7%9A%84%E5%B7%AE%E8%B7%9D%E9%97%AE%E9%A2%98-%E6%A2%AF%E5%BA%A6%E5%86%B2%E7%AA%81-ElasticViT-Conflict-aware-Supernet-Training-for-Deploying-Fast-Vision-Transformer-on-Diverse-Mobile-Devices"><span class="nav-text">17[超网与子网的差距问题-梯度冲突]ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-%E8%87%AA%E9%80%82%E5%BA%94%E9%87%8F%E5%8C%96-Mixed-precision-quantization-of-transformer-language-models-for-speech-recognition"><span class="nav-text">18[自适应量化]Mixed precision quantization of transformer language models for speech recognition </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#19-%E5%8A%A0%E5%85%A5%E5%85%88%E9%AA%8C-%E6%9E%B6%E6%9E%84%E3%80%81%E6%95%B0%E6%8D%AE%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B-Transfer-NAS-with-Meta-learned-Bayesian-Surrogates"><span class="nav-text">19[加入先验-架构、数据表达能力]Transfer NAS with Meta-learned Bayesian Surrogates </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#20-%E6%97%A0%E9%9C%80%E8%AE%AD%E7%BB%83-%E7%BD%91%E7%BB%9C%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%8C%87%E6%A0%87-2202-11921-Auto-scaling-Vision-Transformers-without-Training-%E2%80%A6"><span class="nav-text">20[无需训练-网络复杂度指标][2202.11921] Auto-scaling Vision Transformers without Training … </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#21-%E4%BD%8E%E8%B5%84%E6%BA%90-%E6%B7%B7%E5%90%88%E6%9E%B6%E6%9E%84-Alternative-non-BERT-model-choices-for-the-textual-classification-in-low-resource-languages-and-environments"><span class="nav-text">21[低资源-混合架构]Alternative non-BERT model choices for the textual classification in low-resource languages and environments </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#22-%E5%89%AA%E6%9E%9D-%E5%A4%9A%E7%9B%AE%E6%A0%87-SwiftPruner-Reinforced-Evolutionary-Pruning-for-Efficient-Ad-Relevance"><span class="nav-text">22[剪枝-多目标]SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#23-PLM-NAS-%E6%B7%B7%E5%90%88%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4-%E9%87%8D%E7%94%A8%E5%8F%82%E6%95%B0-Autobert-zero-Evolving-bert-backbone-from-scratch"><span class="nav-text">23[PLM+NAS-混合搜索空间-重用参数]Autobert-zero: Evolving bert backbone from scratch </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#24-%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB%E5%AF%BC%E8%87%B4%E7%9A%84%E6%8E%92%E5%90%8D%E9%97%AE%E9%A2%98-%E5%89%AA%E6%9E%9D-%E6%9E%B6%E6%9E%84%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B-Analyzing-and-mitigating-interference-in-neural-architecture-search"><span class="nav-text">24[权重共享导致的排名问题-剪枝-架构表达能力]Analyzing and mitigating interference in neural architecture search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#25-%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB-%E6%B8%90%E8%BF%9B%E5%BC%8F-Searching-a-high-performance-feature-extractor-for-text-recognition-network"><span class="nav-text">25[文本识别-渐进式]Searching a high performance feature extractor for text recognition network </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#26-%E6%B8%90%E8%BF%9B%E5%BC%8F-Automated-progressive-learning-for-efficient-training-of-vision-transformers"><span class="nav-text">26[渐进式]Automated progressive learning for efficient training of vision transformers </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#27-%E4%BD%8E%E8%B5%84%E6%BA%90-%E6%B7%B7%E5%90%88%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4-%E5%BC%82%E6%AD%A5%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB-ShiftAddNAS-Hardware-inspired-search-for-more-accurate-and-efficient-neural-networks"><span class="nav-text">27[低资源-混合搜索空间-异步权重共享]ShiftAddNAS: Hardware-inspired search for more accurate and efficient neural networks </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#28-%E6%97%A0%E9%9C%80%E8%AE%AD%E7%BB%83-%E7%AA%81%E8%A7%A6%E5%A4%9A%E6%A0%B7%E6%80%A7%E5%92%8C%E6%98%BE%E8%91%97%E6%80%A7%E4%BD%9C%E4%B8%BA%E6%8C%87%E6%A0%87-Training-free-transformer-architecture-search"><span class="nav-text">28[无需训练-突触多样性和显著性作为指标]Training-free transformer architecture search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#29-%E6%9C%80%E4%BC%98%E5%AD%90%E7%BD%91-Vision-transformer-slimming-Multi-dimension-searching-in-continuous-optimization-space"><span class="nav-text">29[最优子网]Vision transformer slimming: Multi-dimension searching in continuous optimization space </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#30-%E5%89%AA%E6%9E%9D-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%BF%9B%E5%8C%96-%E6%9D%83%E9%87%8D%E9%87%8D%E6%9E%84-EAPruning-Evolutionary-Pruning-for-Vision-Transformers-and-CNNs"><span class="nav-text">30[剪枝-多目标进化-权重重构]EAPruning: Evolutionary Pruning for Vision Transformers and CNNs </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#31-%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4-Searching-for-BurgerFormer-with-micro-meso-macro-space-design"><span class="nav-text">31[搜索空间]Searching for BurgerFormer with micro-meso-macro space design </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#32-%E7%BB%9F%E4%B8%80-UFO-unified-feature-optimization"><span class="nav-text">32[统一]UFO: unified feature optimization </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#33-%E7%BB%9F%E4%B8%80-Uninet-Unified-architecture-search-with-convolution-transformer-and-mlp"><span class="nav-text">33[统一]Uninet: Unified architecture search with convolution, transformer, and mlp </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#34-%E6%97%A0%E9%9C%80%E8%AE%AD%E7%BB%83-LiteTransformerSearch-Training-free-Neural-Architecture-Search-for-Efficient-Language-Models"><span class="nav-text">34[无需训练]LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#35-Searching-for-Better-Spatio-temporal-Alignment-in-Few-Shot-Action-Recognition"><span class="nav-text">35[]Searching for Better Spatio-temporal Alignment in Few-Shot Action Recognition </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#36-Autotransformer-Automatic-transformer-architecture-design-for-time-series-classification"><span class="nav-text">36[]Autotransformer: Automatic transformer architecture design for time series classification </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#37-%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB-EEG-based-emotion-recognition-via-transformer-neural-architecture-search"><span class="nav-text">37[情绪识别]EEG-based emotion recognition via transformer neural architecture search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#38-%E6%B0%B4%E4%B8%8B%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-AutoEnhancer-Transformer-on-U-Net-Architecture-Search-for-Underwater-Image-Enhancement"><span class="nav-text">38[水下图像增强]AutoEnhancer: Transformer on U-Net Architecture Search for Underwater Image Enhancement </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#39-%E8%AE%AD%E7%BB%83-Nasvit-Neural-architecture-search-for-efficient-vision-transformers-with-gradient-conflict-aware-supernet-training"><span class="nav-text">39[训练]Nasvit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#40-%E5%9F%BA%E5%87%86-Nas-bench-nlp-neural-architecture-search-benchmark-for-natural-language-processing"><span class="nav-text">40[基准]Nas-bench-nlp: neural architecture search benchmark for natural language processing </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#41-ViTAS-Vision-transformer-architecture-search"><span class="nav-text">41[]ViTAS: Vision transformer architecture search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#42-DARTFormer-Finding-The-Best-Type-Of-Attention"><span class="nav-text">42[]DARTFormer: Finding The Best Type Of Attention </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#43-%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB-NAS-SCAE-Searching-Compact-Attention-based-Encoders-For-End-to-end-Automatic-Speech-Recognition"><span class="nav-text">43[语音识别]NAS-SCAE: Searching Compact Attention-based Encoders For End-to-end Automatic Speech Recognition </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#44-%E6%B3%A8%E6%84%8F%E5%8A%9B-Neural-Architecture-Search-on-Efficient-Transformers-and-Beyond"><span class="nav-text">44[注意力]Neural Architecture Search on Efficient Transformers and Beyond </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#46-%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4-FlexiBERT-Are-Current-Transformer-Architectures-too-Homogeneous-and-Rigid"><span class="nav-text">46[搜索空间]FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid? </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#47-%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E5%90%88%E6%88%90-Neural-Architecture-Search-With-a-Lightweight-Transformer-for-Text-to-Image-Synthesis"><span class="nav-text">47[文本到图像合成]Neural Architecture Search With a Lightweight Transformer for Text-to-Image Synthesis </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#48-Lightspeech-Lightweight-and-fast-text-to-speech-with-neural-architecture-search"><span class="nav-text">48[]Lightspeech: Lightweight and fast text to speech with neural architecture search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#49-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Few-shot-Task-agnostic-Neural-Architecture-Search-for-Distilling-Large-%E2%80%A6"><span class="nav-text">49[知识蒸馏]Few-shot Task-agnostic Neural Architecture Search for Distilling Large … </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#50-one-shot-GraViT-E-Gradient-based-Vision-Transformer-Search-with-Entangled-Weights"><span class="nav-text">50[one-shot]GraViT-E: Gradient-based Vision Transformer Search with Entangled Weights </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#51-SpeedLimit-Neural-Architecture-Search-for-Quantized-Transformer-Models"><span class="nav-text">51[]SpeedLimit: Neural Architecture Search for Quantized Transformer Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#52-Efficient-gradient-based-neural-architecture-search-for-end-to-end-ASR"><span class="nav-text">52[]Efficient gradient-based neural architecture search for end-to-end ASR </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#53-AutoNLU-Architecture-Search-for-Sentence-and-Cross-sentence-Attention-Modeling-with-Re-designed-Search-Space"><span class="nav-text">53[]AutoNLU: Architecture Search for Sentence and Cross-sentence Attention Modeling with Re-designed Search Space </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#54-NASPY-A-UTOMATED-E-XTRACTION-OF-A-UTOMATED-M-ACHINE-L-EARNING-M-ODELS"><span class="nav-text">54[]NASPY : A UTOMATED E XTRACTION OF A UTOMATED M ACHINE L EARNING M ODELS </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#55-Searching-for-efficient-multi-stage-vision-transformers"><span class="nav-text">55[]Searching for efficient multi-stage vision transformers </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#56-Searching-for-Efficient-Transformers-for-Language-Modeling"><span class="nav-text">56[]Searching for Efficient Transformers for Language Modeling </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#57-%E7%97%85%E7%81%B6%E5%88%86%E5%89%B2-T-AutoML-Automated-machine-learning-for-lesion-segmentation-using-transformers-in-3d-medical-imaging"><span class="nav-text">57[病灶分割]T-AutoML: Automated machine learning for lesion segmentation using transformers in 3d medical imaging </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#58-Autosumm-Automatic-model-creation-for-text-summarization-https-aclanthology-org-2021-emnlp-main-798-utm-campaign-%E6%AF%8E%E9%80%B1-NLP-%E8%AB%96%E6%96%87-amp-utm-medium-x3D-email-amp-utm-source-x3D-Revue-newsletter"><span class="nav-text">58[][Autosumm: Automatic model creation for text summarization](https:&#x2F;&#x2F;aclanthology.org&#x2F;2021.emnlp-main.798&#x2F;?utm_campaign&#x3D;毎週  NLP 論文&amp;utm_medium&#x3D;email&amp;utm_source&#x3D;Revue newsletter)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#59-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%AF%86%E9%9B%86%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1-NASformer-Neural-architecture-search-for-vision-transformer"><span class="nav-text">59[图像分类和密集预测任务]NASformer: Neural architecture search for vision transformer </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#60-Searching-the-search-space-of-vision-transformer"><span class="nav-text">60[]Searching the search space of vision transformer </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#62-%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4-%E5%AF%86%E9%9B%86%E9%A2%84%E6%B5%8B-HR-NAS-Searching-Efficient-High-Resolution-Neural-Architectures-with-Lightweight-Transformers"><span class="nav-text">62[搜索空间 密集预测]HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#63-%E5%A4%9A%E6%A8%A1%E6%80%81-BM-NAS-Bilevel-Multimodal-Neural-Architecture-Search"><span class="nav-text">63[多模态]BM-NAS: Bilevel Multimodal Neural Architecture Search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#64-%E6%9E%B6%E6%9E%84%E7%BC%96%E7%A0%81-CATE-Computation-aware-Neural-Architecture-Encoding-with-Transformers"><span class="nav-text">64[架构编码]CATE: Computation-aware Neural Architecture Encoding with Transformers </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#65-%E5%A4%9A%E6%A8%A1%E6%80%81-MUFASA-Multimodal-Fusion-Architecture-Search-for-Electronic-Health-Records"><span class="nav-text">65[多模态]MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#66-%E6%9E%B6%E6%9E%84-TNASP-A-Transformer-based-NAS-Predictor-with-a-Self-evolution-Framework"><span class="nav-text">66[架构]TNASP: A Transformer-based NAS Predictor with a Self-evolution Framework </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#67-%E9%AB%98%E5%85%89%E8%B0%B1%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-Spectral-Spatial-Transformer-Network-for-Hyperspectral-Image-Classification-A-Factorized-Architecture-Search-Framework"><span class="nav-text">67[高光谱图像分类]Spectral-Spatial Transformer Network for Hyperspectral Image Classification: A Factorized Architecture Search Framework </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#68-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Automatic-Student-Network-Search-for-Knowledge-Distillation"><span class="nav-text">68[知识蒸馏]Automatic Student Network Search for Knowledge Distillation </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#69-Glit-Neural-architecture-search-for-global-and-local-image-transformer"><span class="nav-text">69[]Glit: Neural architecture search for global and local image transformer </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#70-%E5%AF%B9%E6%AF%94-x2F-%E9%9B%86%E6%88%90-Bossnas-Exploring-hybrid-cnn-transformers-with-block-wisely-self-supervised-neural-architecture-search"><span class="nav-text">70[对比&#x2F;集成]Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#71-Autoformer-Searching-transformers-for-visual-recognition"><span class="nav-text">71[]Autoformer: Searching transformers for visual recognition </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#72-NAS-BERT-task-agnostic-and-adaptive-size-BERT-compression-with-neural-architecture-search"><span class="nav-text">72[]NAS-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#73-%E6%9E%B6%E6%9E%84%E4%BC%98%E5%8C%96-Towards-Accurate-and-Compact-Architectures-via-Neural-Architecture-Transformer-Transformer%E5%BA%94%E7%94%A8%E4%BA%8E-NAS"><span class="nav-text">73[架构优化]Towards Accurate and Compact Architectures via Neural Architecture Transformer (Transformer应用于*NAS)*</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#74-Autotrans-Automating-transformer-design-via-reinforced-architecture-search"><span class="nav-text">74[]Autotrans: Automating transformer design via reinforced architecture search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#75-PLM-AutoTinyBERT-Automatic-Hyper-parameter-Optimization-for-Efficient-Pre-%E2%80%A6"><span class="nav-text">75[PLM]AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre … </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#76-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Accelerating-Neural-Architecture-Search-for-Natural-Language-Processing-with-Knowledge-Distillation-and-Earth-Mover%E2%80%99s-Distance"><span class="nav-text">76[知识蒸馏]Accelerating Neural Architecture Search for Natural Language Processing with Knowledge Distillation and Earth Mover’s Distance </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#77-RankNAS-Efficient-Neural-Architecture-Search-by-Pairwise-Ranking"><span class="nav-text">77[]RankNAS: Efficient Neural Architecture Search by Pairwise Ranking </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#78-Memory-Efficient-Differentiable-Transformer-Architecture-Search"><span class="nav-text">78[]Memory-Efficient Differentiable Transformer Architecture Search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#79-Neural-architecture-search-algorithm-to-optimize-deep-Transformer-model-for-fault-detection-in-electrical-power-distribution-systems"><span class="nav-text">79[]Neural architecture search algorithm to optimize deep Transformer model for fault detection in electrical power distribution systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#81-HCT-net-hybrid-CNN-transformer-model-based-on-a-neural-architecture-search-network-for-medical-image-segmentation"><span class="nav-text">81[]HCT-net: hybrid CNN-transformer model based on a neural architecture search network for medical image segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#82-AutoTaskFormer-Searching-Vision-Transformers-for-Multi-task-Learning"><span class="nav-text">82[]AutoTaskFormer: Searching Vision Transformers for Multi-task Learning </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#83-%E7%A1%AC%E4%BB%B6-Hyperscale-Hardware-Optimized-Neural-Architecture-Search"><span class="nav-text">83[硬件]Hyperscale Hardware Optimized Neural Architecture Search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#84-%E5%9F%BA%E5%87%86-Neural-architecture-search-as-multiobjective-optimization-benchmarks-Problem-formulation-and-performance-assessment"><span class="nav-text">84[基准]Neural architecture search as multiobjective optimization benchmarks: Problem formulation and performance assessment </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#85-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Neural-Architecture-Search-for-Effective-Teacher-Student-Knowledge-Transfer-in-Language-Models"><span class="nav-text">85[知识蒸馏]Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#87-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-Towards-real-time-segmentation-on-the-edge"><span class="nav-text">87[语义分割]Towards real-time segmentation on the edge </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#88-LLMatic-Neural-Architecture-Search-via-Large-Language-Models-and-Quality-Diversity-Optimization"><span class="nav-text">88[]LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#89-https-arxiv-org-pdf-2206-01191-pdf"><span class="nav-text">89[]https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2206.01191.pdf </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#90-https-arxiv-org-pdf-2212-08059-pdf"><span class="nav-text">90[]https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.08059.pdf </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#91-Multi-dimensional-vision-transformer-compression-via-dependency-guided-gaussian-process-search"><span class="nav-text">91[]Multi-dimensional vision transformer compression via dependency guided gaussian process search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#92-Sparse-structure-search-for-delta-tuning"><span class="nav-text">92[]Sparse structure search for delta tuning </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#93-%E6%8E%A8%E8%8D%90-Towards-automatic-discovering-of-deep-hybrid-network-architecture-for-sequential-recommendation"><span class="nav-text">93[推荐]Towards automatic discovering of deep hybrid network architecture for sequential recommendation </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#94-Neural-prompt-search"><span class="nav-text">94[]Neural prompt search </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#95-%E5%9B%BE-HGNAS-efficient-architecture-search-for-heterogeneous-graph-neural-networks"><span class="nav-text">95[图]HGNAS++: efficient architecture search for heterogeneous graph neural networks </span></a></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2023</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Shuai Lv</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        VISITOR COUNT&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        TOTAL PAGE VIEWS&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a></span>
                <br>
            <span class="theme-version-container">THEME&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.1.4</a>
        </div>
        
        
        
            <div id="start_div" style="display:none">
                2023/5/28 22:00:00
            </div>
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <div class="customize-info info-item">ChatGPT is all you need.</div>
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    


</main>



<script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/utils.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/main.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/layouts/navbarShrink.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/scrollTopBottom.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/lightDarkSwitch.js"></script>




    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/codeBlock.js"></script>



    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/layouts/lazyload.js"></script>



    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/runtime.js"></script>
    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/layouts/odometer.min.js"></script>
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/assets/odometer-theme-minimal.css">



  <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/libs/Typed.min.js"></script>
  <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/plugins/typed.js"></script>



    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/libs/mermaid.min.js"></script>
    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/plugins/mermaid.js"></script>



    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/libs/minimasonry.min.js"></script>
    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/plugins/masonry.js"></script>


<div class="post-scripts pjax">
    
        <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/tocToggle.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/libs/anime.min.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/layouts/toc.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/plugins/tabs.js"></script>
    
</div>


    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/libs/pjax.min.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




    <div id="aplayer"></div>
<script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/libs/APlayer.min.js"></script>
<script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/plugins/aplayer.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":200,"height":400,"position":"left","hOffset":30,"vOffset":30},"mobile":{"show":true},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body>
</html>
