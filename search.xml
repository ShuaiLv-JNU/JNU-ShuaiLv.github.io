<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2023-ICLR-Single-shot General Hyper-parameter Optimization for Federated Learning</title>
    <url>/2023/05/29/2023-ICLR-Single-shot-General-Hyper-parameter-Optimization-for-Federated-Learning/</url>
    <content><![CDATA[<ol>
<li>这篇文章提出了一种名为FLoRA（Federated Loss SuRface Aggregation）的通用FL-HPO解决方案框架，它能够解决表格数据的用例，并且能够扩展到任何机器学习模型，包括梯度提升训练算法、SVM、神经网络等。FLoRA实现了单次FL-HPO：确定一组好的超参数，然后在单次FL训练中使用。因此，它能够在与不进行HPO的FL训练相比，以最小的额外通信开销实现FL-HPO解决方案。在理论上，我们针对任何凸和非凸损失函数，对FLoRA的最优性差进行了表征，这显式地考虑了各方本地数据分布的异构性质，这是FL系统的一个主要特征。我们对FLoRA在七个OpenML数据集上针对多个FL算法的实证评估表明，与基线相比，模型精度显著提高，并且对参与FL-HPO训练的各方数量增加具有鲁棒性。</li>
</ol>
<h1 id="1-概览"><a href="#1-概览" class="headerlink" title="1.概览"></a>1.概览</h1><ol>
<li><strong>提出了什么方法，利用了什么技术，实现了什么效果？</strong>提出了一种名为FLoRA（Federated Loss SuRface Aggregation）的通用FL-HPO解决方案框架，能够解决表格数据的用例，并且能够扩展到任何机器学习模型，包括梯度提升训练算法、SVM、神经网络等。FLoRA实现了单次（single-shot）FL-HPO：确定一组好的超参数，然后在单次FL训练中使用。因此，它能够在与不进行HPO的FL训练相比，以最小的额外通信开销实现FL-HPO解决方案。在理论上，我们针对任何凸和非凸损失函数，对FLoRA的最优性差进行了表征，这显式地考虑了各方本地数据分布的异构性质，这是FL系统的一个主要特征。我们对FLoRA在七个OpenML数据集上针对多个FL算法的实证评估表明，与基线相比，模型精度显著提高，并且对参与FL-HPO训练的各方数量增加具有鲁棒性。</li>
<li><strong>相比过去的方案有哪些优势，解决了什么过去的方法解决不了的问题？</strong>与以前的方法相比，他们的解决方案具有以下优点：它更通用，因为它可以调整多个超参数，并且适用于非SGD训练设置，例如梯度提升树。这是通过将FL-HPO视为黑盒HPO问题实现的（与灰盒HPO相反，我们可以利用诸如权重共享和多保真度HPO之类的技术），这在集中式HPO文献中已经使用网格搜索、随机搜索和贝叶斯优化方法进行了解决。（1）关键挑战在于需要对大量的HPO配置执行计算密集型的评估，其中每次评估都涉及训练一个模型并在验证数据集上对其进行评分。在分布式FL设置中，这个问题更加严峻，因为验证集是各方本地的，而且每次FL训练/评分评估都需要大量的通信。因此，直接应用集中式黑盒HPO方法，在外循环中选择HP并继续进行FL训练评估是不可行的。（2）它产生最小的HPO通信开销。这是通过从各方本地异步HPO构建损失曲面来实现的，该损失曲面产生单个优化的HP配置，用于训练单个全局模型。 （3）它是第一个在FL-HPO设置中理论上表征最优性差距的方法，适用于我们在本文中关注的情况：通过调整多个全局HP而不访问全局验证数据集来创建全局模型。</li>
<li><strong>方法的main procedure：</strong>FLoRA算法主要过程如下：首先，每个参与方<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.192ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 969 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>都会运行HPO以生成T（超参数、损失）对<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="26.809ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 11849.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(764,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1153,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1498,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2164.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3220.6,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(858,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1247,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1592,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(1981,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2342,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2786.7,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(3467.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3856.7,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4201.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(4590.7,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(4951.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5340.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(5785.3,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(6424.1,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mo" transform="translate(7368.9,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(7646.9,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(8350.9,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></g></svg></mjx-container>，其中<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="41.472ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 18330.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(469,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(858,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1203,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(1592,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2230.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(3175.6,0)"><path data-c="1D6E9" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM640 466Q640 523 625 565T583 628T532 658T479 668Q370 668 273 559T151 255Q150 245 150 213Q150 156 165 116T207 55T259 26T313 17Q385 17 451 63T561 184Q590 234 615 312T640 466ZM510 276Q510 278 512 288L515 298Q515 299 384 299H253L250 285Q246 271 244 268T231 265H227Q216 265 214 266T207 274Q207 278 223 345T244 416Q247 419 260 419H263Q280 419 280 408Q280 406 278 396L275 386Q275 385 406 385H537L540 399Q544 413 546 416T559 419H563Q574 419 576 418T583 410Q583 403 566 339Q549 271 544 267Q542 265 538 265H530H527Q510 265 510 276Z"></path></g><g data-mml-node="mo" transform="translate(3938.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4383.2,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(5064.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5453.2,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(5798.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(6187.2,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(6826,0)"><g data-mml-node="text"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="text" transform="translate(278,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g></g><g data-mml-node="mi" transform="translate(8159.8,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(8840.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9229.8,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(9979.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(10368.8,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mo" transform="translate(11419.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(11864.4,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(12333.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(12722.4,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(13067.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(13456.4,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(13817.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(14262.1,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(15090.1,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(15435.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(15824.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(16268.8,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mn" transform="translate(17096.8,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mi" transform="translate(17596.8,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(17941.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。<br>然后，在聚合器处收集所有<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="15.963ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7055.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(1041.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2097.6,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(764,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1153,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1498,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(1887,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2331.7,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2954.4,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mo" transform="translate(3899.2,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(4177.2,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(4680.2,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></g></svg></mjx-container>。<br>接下来，使用E生成统一损失曲面<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="9.819ex" height="1.643ex" role="img" focusable="false" viewBox="0 -704 4340.1 726"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(706.8,0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(1262.6,0)"><path data-c="1D6E9" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM640 466Q640 523 625 565T583 628T532 658T479 668Q370 668 273 559T151 255Q150 245 150 213Q150 156 165 116T207 55T259 26T313 17Q385 17 451 63T561 184Q590 234 615 312T640 466ZM510 276Q510 278 512 288L515 298Q515 299 384 299H253L250 285Q246 271 244 268T231 265H227Q216 265 214 266T207 274Q207 278 223 345T244 416Q247 419 260 419H263Q280 419 280 408Q280 406 278 396L275 386Q275 385 406 385H537L540 399Q544 413 546 416T559 419H563Q574 419 576 418T583 410Q583 403 566 339Q549 271 544 267Q542 265 538 265H530H527Q510 265 510 276Z"></path></g><g data-mml-node="mo" transform="translate(2303.3,0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3581.1,0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g></g></svg></mjx-container>。<br>然后选择最佳超参数候选<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="23.385ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 10336.1 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mi" transform="translate(469,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(898,0)"><path data-c="3F" d="M226 668Q190 668 162 656T124 632L114 621Q116 621 119 620T130 616T145 607T157 591T162 567Q162 544 147 529T109 514T71 528T55 566Q55 625 100 661T199 704Q201 704 210 704T224 705H228Q281 705 320 692T378 656T407 612T416 567Q416 503 361 462Q267 395 247 303Q242 279 242 241V224Q242 205 239 202T222 198T205 201T202 218V249Q204 320 220 371T255 445T292 491T315 537Q317 546 317 574V587Q317 604 315 615T304 640T277 661T226 668ZM162 61Q162 89 180 105T224 121Q247 119 264 104T281 61Q281 31 264 16T222 1Q197 1 180 16T162 61Z"></path></g><g data-mml-node="mo" transform="translate(1647.8,0)"><path data-c="2190" d="M944 261T944 250T929 230H165Q167 228 182 216T211 189T244 152T277 96T303 25Q308 7 308 0Q308 -11 288 -11Q281 -11 278 -11T272 -7T267 2T263 21Q245 94 195 151T73 236Q58 242 55 247Q55 254 59 257T73 264Q121 283 158 314T215 375T247 434T264 480L267 497Q269 503 270 505T275 509T288 511Q308 511 308 500Q308 493 303 475Q293 438 278 406T246 352T215 315T185 287T165 270H929Q944 261 944 250Z"></path></g><g data-mml-node="mi" transform="translate(2925.6,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(3454.6,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3905.6,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(4382.6,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5260.6,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5605.6,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(6205.6,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(6952.3,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(7897.1,0)"><path data-c="1D6E9" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM640 466Q640 523 625 565T583 628T532 658T479 668Q370 668 273 559T151 255Q150 245 150 213Q150 156 165 116T207 55T259 26T313 17Q385 17 451 63T561 184Q590 234 615 312T640 466ZM510 276Q510 278 512 288L515 298Q515 299 384 299H253L250 285Q246 271 244 268T231 265H227Q216 265 214 266T207 274Q207 278 223 345T244 416Q247 419 260 419H263Q280 419 280 408Q280 406 278 396L275 386Q275 385 406 385H537L540 399Q544 413 546 416T559 419H563Q574 419 576 418T583 410Q583 403 566 339Q549 271 544 267Q542 265 538 265H530H527Q510 265 510 276Z"></path></g><g data-mml-node="mi" transform="translate(8660.1,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(9089.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9478.1,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(9947.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。<br>最后，调用联合训练<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="21.026ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 9293.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1155.8,0)"><path data-c="2190" d="M944 261T944 250T929 230H165Q167 228 182 216T211 189T244 152T277 96T303 25Q308 7 308 0Q308 -11 288 -11Q281 -11 278 -11T272 -7T267 2T263 21Q245 94 195 151T73 236Q58 242 55 247Q55 254 59 257T73 264Q121 283 158 314T215 375T247 434T264 480L267 497Q269 503 270 505T275 509T288 511Q308 511 308 500Q308 493 303 475Q293 438 278 406T246 352T215 315T185 287T165 270H929Q944 261 944 250Z"></path></g><g data-mml-node="mi" transform="translate(2433.6,0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mo" transform="translate(3182.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3571.6,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mo" transform="translate(4622.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(5067.2,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mi" transform="translate(5536.2,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(5965.2,0)"><path data-c="3F" d="M226 668Q190 668 162 656T124 632L114 621Q116 621 119 620T130 616T145 607T157 591T162 567Q162 544 147 529T109 514T71 528T55 566Q55 625 100 661T199 704Q201 704 210 704T224 705H228Q281 705 320 692T378 656T407 612T416 567Q416 503 361 462Q267 395 247 303Q242 279 242 241V224Q242 205 239 202T222 198T205 201T202 218V249Q204 320 220 371T255 445T292 491T315 537Q317 546 317 574V587Q317 604 315 615T304 640T277 661T226 668ZM162 61Q162 89 180 105T224 121Q247 119 264 104T281 61Q281 31 264 16T222 1Q197 1 180 16T162 61Z"></path></g><g data-mml-node="mo" transform="translate(6437.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(6881.9,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(7631.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(8076.6,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mo" transform="translate(8904.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。</li>
<li><strong>方法在什么任务上，实现了什么性能？</strong>在实验部分，作者使用FLoRA对多种机器学习模型（HGB、SVM和MLP）在OpenML分类问题上进行了FL-HPO。与单次基线相比，所有FLoRA损失曲面都表现出强大的性能，具有明显更多的胜利而不是损失，并且第三四分位相对遗憾值小于1（表示优于基线）。所有FLoRA损失曲面的p值均小于0.05，表明我们可以拒绝零假设。总体而言，APLM在所有损失曲面中表现最佳，无论是在胜/平/负方面还是在威尔科克森符号秩检验方面，都具有最高的统计量和接近10−3的p值。APLM还具有明显低于所有其他损失曲面的第三四分位数。</li>
<li><strong>结论：</strong>根据结论部分，这篇文章提出了一种新颖的FL-HPO框架FLoRA，它利用元学习技术使各方能够在各自进行异步本地HPO以执行单次HPO以解决全局FL-HPO问题。作者提供了理论保证，涵盖了IID和Non-IID情况，无论损失函数的凸性如何。他们对FLoRA在OpenML上七个分类数据集上针对多个FL算法的实证评估表明，与基线相比，模型精度显著提高，并且对参与FL-HPO训练的各方数量增加具有鲁棒性。</li>
</ol>
<h1 id="2-精读"><a href="#2-精读" class="headerlink" title="2.精读"></a>2.精读</h1><p>——</p>
]]></content>
      <tags>
        <tag>PaperReading</tag>
      </tags>
  </entry>
  <entry>
    <title>2022|Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications|韩松|CCF-B</title>
    <url>/2023/05/28/2022-Enable-Deep-Learning-on-Mobile-Devices-Methods-Systems-and-Applications-%E9%9F%A9%E6%9D%BE-CCF-B/</url>
    <content><![CDATA[<blockquote>
<p>链接：<a class="link"   href="https://arxiv.org/pdf/2204.11786.pdf" >https://arxiv.org/pdf/2204.11786.pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
</blockquote>
<h1 id="1-总结"><a href="#1-总结" class="headerlink" title="1.总结"></a>1.总结</h1><p>本文介绍了一种用于在移动设备上实现深度学习的方法、系统和应用。文章从介绍流行的模型压缩方法开始，包括剪枝、分解、量化以及紧凑模型设计。为了减少这些手动解决方案的大量设计成本，讨论了每个人的AutoML框架，如<strong>神经架构搜索（NAS）</strong>和<strong>自动剪枝和量化</strong>。然后涵盖了高效的设备内训练，以便在移动设备上基于本地数据实现用户定制。除了通用加速技术，还展示了通过利用它们的空间稀疏性和时间&#x2F;令牌冗余来加速点云、视频和自然语言处理的几种任务特定加速。最后，为了支持所有这些算法进展，从软件和硬件角度介绍了高效深度学习系统设计。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/ShuaiLv-JNU/blogImage/main/img/202305282317848.png"
                     
                ></p>
<span id="more"></span>

<h1 id="2-优势"><a href="#2-优势" class="headerlink" title="2.优势"></a>2.优势</h1><p>与以前的解决方案相比，本文旨在涵盖更广泛的高效深度学习方法和应用：从手动到自动，从新原语&#x2F;操作设计到设计空间探索，从训练到推理，从算法到硬件，从通用目的到应用特定优化。</p>
<h1 id="3-细节"><a href="#3-细节" class="headerlink" title="3.细节"></a>3.细节</h1><h2 id="3-1剪枝"><a href="#3-1剪枝" class="headerlink" title="3.1剪枝"></a>3.1剪枝</h2><p>DNN通常是过度参数化的，修剪可以去除神经网络中的冗余元素，以减少模型大小和计算成本。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/ShuaiLv-JNU/blogImage/main/img/202305282324146.png"
                     
                ></p>
<p><strong>Step1：选择修剪粒度</strong></p>
<ul>
<li><p>颗粒度修剪：</p>
<ul>
<li><p>去除权重张量中的个别元素（细粒度）</p>
</li>
<li><p>基于模式的修剪（细粒度）</p>
</li>
<li><p>通道修剪（粗粒度）</p>
</li>
</ul>
</li>
<li><p>硬件加速</p>
</li>
</ul>
<p><strong>Step2：重要性标准</strong></p>
<p>确定哪些权重要被修剪对于修剪后的模型性能也是至关重要的。在模型训练后，有几种重要性标准的启发式方法来估计每个权重的重要性。</p>
<p><strong>Step3：训练方法</strong></p>
<p>在大的压缩比下，直接删除深度神经网络中的权重将大大降低准确性。因此，需要进行一些训练&#x2F;微调来恢复性能损失。微调可以在修剪后进行，以恢复性能下降。</p>
<h2 id="3-2低秩因子化"><a href="#3-2低秩因子化" class="headerlink" title="3.2低秩因子化"></a>3.2低秩因子化</h2><p>使用矩阵&#x2F;张量分解来降低深度神经网络中卷积或全连接层的复杂性，使用低秩过滤器来加速卷积的想法在信号处理领域已被长期研究，比如SVD。</p>
<h2 id="3-3量化"><a href="#3-3量化" class="headerlink" title="3.3量化"></a>3.3量化</h2><p>通过减少表示深层网络所需的每个权重的比特来压缩网络。在硬件支持下，量化后的网络可以有更快的推理速度。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/ShuaiLv-JNU/blogImage/main/img/202305282333445.png"
                     
                ></p>
<ul>
<li><strong>四舍五入方案</strong>：为了将全精度权重(32位浮点值)量化为低精度，四舍五入被用来将浮点值映射到一个量化桶中。<ul>
<li>k-means：落入同一类的共享权重</li>
<li>线性&#x2F;统一量化：范围截断后直接将浮点值四舍五入为最接近的量化值。</li>
</ul>
</li>
<li><strong>位精度</strong>：使用不同的位来权衡模型大小，较低的位精度模型小，但精度下降也大。全精度权重和激活都使用FP32。半精度使用FP16。INT8常用于GPU加速。</li>
<li><strong>量化方案</strong>：①对于更高精度的量化( 如INT8)，可以进行<strong>训练后量化</strong>，在全精度模型训练后对权重和激活进行量化，激活的量化范围是通过计算训练集上的分布来确定的。应用训练后的INT8量化通常会导致精度的微小损失或没有损失。最近的工作也研究了INT4模型的训练后量化问题。②<strong>量化感知训练</strong>可以通过在训练期间模拟推理时间量化来减少量化精度的损失，训练期间的前向传递与测试时间一致，这有助于设备上的部署。</li>
</ul>
<h2 id="3-4-知识蒸馏"><a href="#3-4-知识蒸馏" class="headerlink" title="3.4 知识蒸馏"></a>3.4 知识蒸馏</h2><p>KD将大模型（教师）中学习的“暗知识”转移到小模型（学生）中。</p>
<h2 id="3-5紧凑模型设计"><a href="#3-5紧凑模型设计" class="headerlink" title="3.5紧凑模型设计"></a>3.5紧凑模型设计</h2><ul>
<li>Mobile net</li>
<li>shuffle net：引入了两个新的操作，顺时针分组卷积和channel shuffle。</li>
<li>squeeze net</li>
</ul>
<h2 id="3-6NAS"><a href="#3-6NAS" class="headerlink" title="3.6NAS"></a>3.6NAS</h2><p>——</p>
<h2 id="3-7自动模型压缩"><a href="#3-7自动模型压缩" class="headerlink" title="3.7自动模型压缩"></a>3.7自动模型压缩</h2><p>模型压缩方法可以提高部署模型的效率。然而，模型压缩的性能主要受超参数的影响。例如，深度网络中的不同层有不同的能力和敏感度( 例如，CNN中的第一层通常对修剪非常敏感 )。因此我们应该<strong>对网络的不同层应用不同的修剪比例</strong>，以达到最佳性能。设计空间如此之大，以至于人类的启发式方法通常是次优的，而且人工模型压缩也很耗时。为此，提出了自动模型压缩，以找到好的压缩策略。</p>
<ul>
<li><p><strong>自动剪枝</strong>：传统的模型修剪技术依赖于手工制作的特征，需要领域专家探索庞大的设计空间在模型大小、速度和精度之间进行权衡，这通常是次优的，而且很费时间。AMC利用强化学习来有效地对设计空间进行采样，并为给定的网络找到最佳的剪枝策略。奖励被计算为准确性和FLOP的函数；MetaPruning首先训练了一个PruningNet，一种<strong>元网络</strong>，它能够为给定的目标网络的任何修剪结构生成权重参数，然后用它来搜索不同约束条件下的最佳修剪策略。该元网络可用于直接测量压缩精度，而无需进行微调。</p>
</li>
<li><p><strong>自动量化</strong>：混合精度量化也需要大量的努力来决定每一层的最佳位宽，以实现最佳的精度-性能权衡。硬件感知自动量化(HAQ)利用强化学习来自动确定量化策略；一种用于混合精度量化的二阶量化方法Hessian AWare Quantization ( HAWQ )允许根据各层的Hessian谱，自动选择各层的相对量化精度，当应用于大型语言模型时，还显示出卓越的性能。</p>
</li>
</ul>
<h2 id="3-8联合压缩与NAS"><a href="#3-8联合压缩与NAS" class="headerlink" title="3.8联合压缩与NAS"></a>3.8联合压缩与NAS</h2><ul>
<li>顺序优化：即分别应用这两种技术。</li>
<li>联合优化（端到端）</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/ShuaiLv-JNU/blogImage/main/img/202305282353765.png"
                     
                ></p>
<h2 id="3-9高效的设备上学习"><a href="#3-9高效的设备上学习" class="headerlink" title="3.9高效的设备上学习"></a>3.9高效的设备上学习</h2><ul>
<li>梯度检查点</li>
<li>激活修剪</li>
<li>低比特训练</li>
<li>梯度压缩</li>
</ul>
<h2 id="3-10高效的迁移学习"><a href="#3-10高效的迁移学习" class="headerlink" title="3.10高效的迁移学习"></a>3.10高效的迁移学习</h2><p>——</p>
<h2 id="3-11联邦学习"><a href="#3-11联邦学习" class="headerlink" title="3.11联邦学习"></a>3.11联邦学习</h2><p>联合学习允许多个客户联合训练一个模型，而不需要明确分享他们的数据。随着个人数据的隐私问题越来越受到关注，这也导致了在不破坏隐私的情况下进行训练的需求越来越大。虽然在部署中拥有许多边缘设备是很常见的，但联合学习提供了一种利用所有设备的方法，并解决了安全问题，因为本地数据从未离开过客户端。与配备高端网络基础设施的集群不同，边缘设备通常与功能较弱的网络(Wi-Fi)相连在<strong>这种情况下，带宽很低，延迟很高</strong>，传统方法的扩展性很差。为了消除瓶颈，联合平均、梯度压缩和量化大大减少了传输的比特，以降低带宽要求，延迟更新处理了延迟的问题。</p>
]]></content>
      <tags>
        <tag>PaperReading</tag>
      </tags>
  </entry>
  <entry>
    <title>ChatGPT提问模板</title>
    <url>/2023/05/29/ChatGPT%E6%8F%90%E9%97%AE%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[<blockquote>
<p>ref1: <a class="link"   href="https://zhuanlan.zhihu.com/p/610735657" >https://zhuanlan.zhihu.com/p/610735657 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
</blockquote>
<p>Don’t search the Internet, you are now a PhD student in the field of <code>need to change according to paper</code>, and now you need to help me summarize this article according to the following contents (Please answer me in Chinese.): 1. First summarize what method, what technology is used, and what effect is achieved in this paper? 2. What are the advantages of their solution compared with the previous ones, and what problems did they solve that the previous methods could not solve? 3. Please describe the main procedure of the method in detail in combination with the content of the Method section. Please use latex to display the key variables. 4. Combined with the Experiments section, please summarize what task and performance the method achieves? Please list specific values according to this section. 5. Please combine the Conclusion section to summarize what problems still exist in this method?</p>
]]></content>
      <tags>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>资源汇总</title>
    <url>/2023/05/29/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<ul>
<li><h2 id="工具-科研辅助AI软件——inciteful-哔哩哔哩-bilibili-Zotero-GPT-与ChatGPT一起读文献-哔哩哔哩-bilibili-Zotero-利用Research-Rabbit了解研究领域的相关文献，获取同一作者已发表的所有文献-哔哩哔哩-bilibili"><a href="#工具-科研辅助AI软件——inciteful-哔哩哔哩-bilibili-Zotero-GPT-与ChatGPT一起读文献-哔哩哔哩-bilibili-Zotero-利用Research-Rabbit了解研究领域的相关文献，获取同一作者已发表的所有文献-哔哩哔哩-bilibili" class="headerlink" title="工具- 科研辅助AI软件——inciteful_哔哩哔哩_bilibili- Zotero GPT | 与ChatGPT一起读文献_哔哩哔哩_bilibili- Zotero|利用Research Rabbit了解研究领域的相关文献，获取同一作者已发表的所有文献_哔哩哔哩_bilibili"></a><strong>工具</strong><br>- <a class="link"   href="https://www.bilibili.com/video/BV1UM4y1b7w8/?spm_id_from=333.1007.tianma.7-2-24.click&vd_source=5bc699d65b90929607821ea2dff49140" >科研辅助AI软件——inciteful_哔哩哔哩_bilibili <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><br>- <a class="link"   href="https://www.bilibili.com/video/BV1Wa4y1V777/?spm_id_from=333.999.0.0&vd_source=5bc699d65b90929607821ea2dff49140" >Zotero GPT | 与ChatGPT一起读文献_哔哩哔哩_bilibili <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><br>- <a class="link"   href="https://www.bilibili.com/video/BV1rh4y1474B/?spm_id_from=333.1007.tianma.6-4-22.click&vd_source=5bc699d65b90929607821ea2dff49140" >Zotero|利用Research Rabbit了解研究领域的相关文献，获取同一作者已发表的所有文献_哔哩哔哩_bilibili <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h2></li>
<li><p>hexo</p>
<ul>
<li><a class="link"   href="https://blog.csdn.net/Awt_FuDongLai/article/details/107424098" >(213条消息) hexo笔记四：next主题添加作者头像_hexo设置自己的头像_小镇攻城狮的博客-CSDN博客 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_38140292/article/details/119076424#:~:text=%E3%80%90hexo%E3%80%91%E5%9F%BA%E7%A1%80,%E5%AE%A2-CSDN%E5%8D%9A%E5%AE%A2" >(212条消息) 【hexo】基础教程-三-添加网易云音乐_指尖听戏的博客-CSDN博客 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link"   href="https://cloud.tencent.com/developer/article/1662733" >Hexo博客教程（二）| 如何写作新文章并发布-腾讯云开发者社区-腾讯云 (tencent.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/350654582" >Hexo-如何养一只看板娘(博客宠物) - 知乎 (zhihu.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><strong><a class="link"   href="https://redefine-docs.ohevan.com/basic/global" >全局功能设置 global - Redefine Docs (ohevan.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></strong></li>
</ul>
</li>
<li><p>论文</p>
<ul>
<li><a class="link"   href="https://www.bilibili.com/video/BV1DL41167ox/?p=2&spm_id_from=pageDriver&vd_source=5bc699d65b90929607821ea2dff49140" >Transformer在医学分割领域应用与拓展_哔哩哔哩_bilibili <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link"   href="https://github.com/Somedaywilldo/BM-NAS/blob/master/structure_vis.ipynb" >BM-NAS&#x2F;structure_vis.ipynb at master · Somedaywilldo&#x2F;BM-NAS · GitHub — BM-NAS&#x2F;structure_vis.ipynb at master ·总有一天会&#x2F;BM-NAS ·GitHub <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li>[<a class="link"   href="https://www.youtube.com/watch?v=EGZu5bOi_M4" >AAAI 2022 Oral] BM-NAS: Bilevel Multimodal Neural Architecture Search [Full] - YouTube <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link"   href="https://arxiv.org/pdf/2008.10937.pdf" >2008.10937.pdf (arxiv.org) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link"   href="https://github.com/Shengcao-Cao/ESNAC" >Shengcao-Cao&#x2F;ESNAC: Learnable Embedding Space for Efficient Neural Architecture Compression (github.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link"   href="https://www.sciencedirect.com/science/article/pii/S0957417422012581" >用于自动多模态学习的神经架构搜索 - ScienceDirect <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
</li>
<li><p>原理</p>
<ul>
<li><a class="link"   href="https://www.bilibili.com/video/BV1ih4y1J7rx/?spm_id_from=333.1007.tianma.6-1-19.click&vd_source=5bc699d65b90929607821ea2dff49140" >超强动画，一步一步深入浅出解释Transformer原理！_哔哩哔哩_bilibili <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li>qiuxipeng的GitHub</li>
</ul>
</li>
<li><p>GitHub page</p>
<ul>
<li><a class="link"   href="https://www.zhihu.com/question/20376047?sort=created" >(19 封私信 &#x2F; 11 条消息) 怎样做一个漂亮的 GitHub Pages 首页？ - 知乎 (zhihu.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
</li>
<li><p>视频</p>
<ul>
<li><a class="link"   href="https://hub.baai.ac.cn/view/21366" >不可错过！MIT韩松博士《TinyML与高效深度学习》2022课程，附Slides - 智源社区 (baai.ac.cn) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
</li>
<li><p>教程</p>
<ul>
<li><a class="link"   href="https://github.com/labmlai" >labml.ai (github.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
</li>
</ul>
<p>不设置任何参数的 <a class="button " href='/' title='按钮'>按钮</a> 适合融入段落中。</p>
<p>regular 按钮适合独立于段落之外：</p>
<a class="button  regular" href='https://www.ohevan.com' title='示例博客'><i class='fa-solid fa-play-circle'></i> 示例博客</a>

<a class="button  regular" href='https://www.ohevan.com' title='示例博客'><i class='fa-solid fa-play-circle'></i> 示例博客</a>

<p>large 按钮更具有强调作用，建议搭配 center 使用：</p>
<a class="button  center large" href='https://redefine-docs.ohevan.com' title='开始使用'><i class='fa-solid fa-download'></i> 开始使用</a>



<blockquote>
<p><a class="link"   href="https://redefine-docs.ohevan.com/modules/notes" >Notes 笔记模块 - Redefine Docs (ohevan.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
</blockquote>
<p><a class="link"   href="https://redefine-docs.ohevan.com/plugins/mermaid" >Mermaid JS 流程图 - Redefine Docs (ohevan.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<ul>
<li>文章置顶：<code>sticky: 999</code>，值越大，置顶的文章越靠前</li>
<li>首页缩略图：<code>thumbnail: &quot;IMAGE_LINK&quot;</code></li>
<li>首页摘要：<code>excerpt: &quot;摘要&quot;</code></li>
<li>文章页头图：<code>cover: &quot;图片链接&quot;</code></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>2023-ICLR-Transfer NAS with Meta-learned Bayesian Surrogates</title>
    <url>/2023/05/30/2023-ICLR-Transfer-NAS-with-Meta-learned-Bayesian-Surrogates/</url>
    <content><![CDATA[<h2 id="1-论文"><a href="#1-论文" class="headerlink" title="1.论文"></a>1.论文</h2><ul>
<li>这篇论文提出了一种新的神经网络结构搜索方法，称为“Transfer NAS with Meta-Learned Bayesian Surrogates”。该方法使用贝叶斯优化（BO）和深度内核高斯过程（DKGP）来进行神经网络结构搜索。此外，该方法还使用图神经网络（GNN）和基于Transformer的数据集编码器来获得架构嵌入和数据集上下文特征。该方法在六个计算机视觉数据集上实现了最先进的结果，并且与黑盒优化方法相比，具有与一次性NAS方法相同数量级的计算成本。此外，该方法还可以在不同数据集之间传递信息，从而提高了效率。</li>
<li>相对于以前的方法，该方法具有以下优点：<ul>
<li>该方法可以在不同数据集之间传递信息，从而提高了效率；</li>
<li>该方法可以利用以前的深度学习经验，从而更快地找到最佳架构；</li>
<li>该方法可以在不同数据集上进行元学习，从而更好地适应新任务。</li>
</ul>
</li>
<li>该方法的主要步骤如下：<ul>
<li>首先，使用GNN将神经网络结构嵌入到向量空间中；</li>
<li>然后，使用Transformer编码器将数据集上下文特征编码为向量；</li>
<li>接下来，使用DKGP学习嵌入空间中架构和数据集之间的内核；</li>
<li>最后，在嵌入空间中使用BO进行优化。</li>
</ul>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/ShuaiLv-JNU/blogImage/main/img/202305302252677.png"
                     
                ></p>
<ul>
<li>在实验部分，作者将其方法与其他NAS方法进行了比较，并在六个计算机视觉数据集上实现了最先进的结果。例如，在CIFAR-10数据集上，作者的方法达到了3.5%的测试误差率，并且速度比其他NAS方法快得多。</li>
<li>作者指出，该方法仍存在一些问题。例如，在大型数据集上进行元学习仍然是一个挑战。此外，在某些情况下，该方法可能会受到过拟合的影响。</li>
</ul>
<h2 id="2-代码"><a href="#2-代码" class="headerlink" title="2.代码"></a>2.代码</h2><h3 id="2-1main"><a href="#2-1main" class="headerlink" title="2.1main"></a>2.1main</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###########################################################################################</span></span><br><span class="line"><span class="comment"># Copyright (c) Hayeon Lee, Eunyoung Hyung [GitHub MetaD2A], 2021</span></span><br><span class="line"><span class="comment"># Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets, ICLR 2021</span></span><br><span class="line"><span class="comment">###########################################################################################</span></span><br><span class="line"><span class="comment">## Step1:导入相关库函数</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> ReduceLROnPlateau</span><br><span class="line"></span><br><span class="line"><span class="comment"># from parser import get_parser</span></span><br><span class="line"><span class="keyword">from</span> generator <span class="keyword">import</span> Generator</span><br><span class="line"><span class="keyword">from</span> predictor <span class="keyword">import</span> Predictor</span><br><span class="line">sys.path.append(os.getcwd())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">str2bool</span>(<span class="params">v</span>):</span><br><span class="line">  <span class="keyword">return</span> v.lower() <span class="keyword">in</span> [<span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;true&#x27;</span>, <span class="literal">True</span>]<span class="comment"># 判断是否为True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Step2:设置输入参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_parser</span>():</span><br><span class="line">  parser = argparse.ArgumentParser()</span><br><span class="line">  <span class="comment"># general settings</span></span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">333</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--gpu&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;0&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;set visible gpus&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--model_name&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;generator&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;select model [generator|predictor]&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--save-path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;C:\\Users\\gress\\OneDrive\\Documents\\Gresa\\DeepKernelGP\\MetaD2A_nas_bench_201\\results&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;the path of save directory&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--data-path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;C:\\Users\\gress\\OneDrive\\Documents\\Gresa\\DeepKernelGP\\MetaD2A_nas_bench_201\\data&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;the path of save directory&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--save-epoch&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">400</span>,</span><br><span class="line">                      <span class="built_in">help</span>=<span class="string">&#x27;how many epochs to wait each time to save model states&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--max-epoch&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">400</span>, <span class="built_in">help</span>=<span class="string">&#x27;number of epochs to train&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>, <span class="built_in">help</span>=<span class="string">&#x27;batch size for generator&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--graph-data-name&#x27;</span>, default=<span class="string">&#x27;nasbench201&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;graph dataset name&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--nvt&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">7</span>,</span><br><span class="line">                      <span class="built_in">help</span>=<span class="string">&#x27;number of different node types, 7: NAS-Bench-201 including in/out node&#x27;</span>)</span><br><span class="line">  <span class="comment"># set encoder</span></span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--num-sample&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">20</span>, <span class="built_in">help</span>=<span class="string">&#x27;the number of images as input for set encoder&#x27;</span>)</span><br><span class="line">  <span class="comment"># graph encoder</span></span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--hs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">56</span>, <span class="built_in">help</span>=<span class="string">&#x27;hidden size of GRUs&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--nz&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">56</span>, <span class="built_in">help</span>=<span class="string">&#x27;the number of dimensions of latent vectors z&#x27;</span>)</span><br><span class="line">  <span class="comment"># test</span></span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--test&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&#x27;turn on test mode&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--load-epoch&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">400</span>, <span class="built_in">help</span>=<span class="string">&#x27;checkpoint epoch loaded for meta-test&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--data-name&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&#x27;meta-test dataset name&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--num-class&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&#x27;the number of class of dataset&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--num-gen-arch&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">800</span>,</span><br><span class="line">                      <span class="built_in">help</span>=<span class="string">&#x27;the number of candidate architectures generated by the generator&#x27;</span>)</span><br><span class="line">  parser.add_argument(<span class="string">&#x27;--train-arch&#x27;</span>, <span class="built_in">type</span>=str2bool, default=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&#x27;whether to train the searched architecture&#x27;</span>)</span><br><span class="line"></span><br><span class="line">  args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> args</span><br><span class="line"></span><br><span class="line"><span class="comment">## Step3:主函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">  args = get_parser()<span class="comment"># 获取命令行参数</span></span><br><span class="line">  os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>]=args.gpu<span class="comment"># 指定可见的CUDA设备</span></span><br><span class="line">  device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">  torch.cuda.manual_seed(args.seed)<span class="comment"># 设置CUDA种子</span></span><br><span class="line">  torch.manual_seed(args.seed)<span class="comment"># 设置CPU种子</span></span><br><span class="line">  np.random.seed(args.seed)</span><br><span class="line">  random.seed(args.seed)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(args.save_path):</span><br><span class="line">    os.makedirs(args.save_path)</span><br><span class="line">  args.model_path = os.path.join(args.save_path, args.model_name, <span class="string">&#x27;model&#x27;</span>)</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(args.model_path):</span><br><span class="line">    os.makedirs(args.model_path)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> args.model_name == <span class="string">&#x27;generator&#x27;</span>:</span><br><span class="line">    g = Generator(args)</span><br><span class="line">    <span class="keyword">if</span> args.test:</span><br><span class="line">      g.meta_test()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      g.meta_train()</span><br><span class="line">  <span class="keyword">elif</span> args.model_name == <span class="string">&#x27;predictor&#x27;</span>:</span><br><span class="line">    p = Predictor(args)</span><br><span class="line">    <span class="keyword">if</span> args.test:</span><br><span class="line">      p.meta_test()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      p.meta_train()</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&#x27;You should select generator|predictor|train_arch&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">  main()</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h3 id="2-2process-dataset"><a href="#2-2process-dataset" class="headerlink" title="2.2process_dataset"></a>2.2process_dataset</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dset</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> os, sys</span><br><span class="line"><span class="keyword">if</span> sys.version_info[<span class="number">0</span>] == <span class="number">2</span>:</span><br><span class="line">	<span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	<span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(<span class="string">&quot;sota&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--gpu&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;0&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;set visible gpus&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--data-path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;data&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;the path of save directory&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--dataset&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;cifar10&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;choose dataset&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=-<span class="number">1</span>, <span class="built_in">help</span>=<span class="string">&#x27;random seed&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> args.seed &lt; <span class="number">0</span>: args.seed = random.randint(<span class="number">1</span>, <span class="number">100000</span>)</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = args.gpu</span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line">random.seed(args.seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove last fully-connected layer</span></span><br><span class="line">model = models.resnet18(pretrained=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line">feature_extractor = torch.nn.Sequential(*<span class="built_in">list</span>(model.children())[:-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_transform</span>(<span class="params">dataset</span>):</span><br><span class="line">	<span class="keyword">if</span> args.dataset == <span class="string">&#x27;mnist&#x27;</span>:</span><br><span class="line">		mean, std = [<span class="number">0.1307</span>, <span class="number">0.1307</span>, <span class="number">0.1307</span>], [<span class="number">0.3081</span>, <span class="number">0.3081</span>, <span class="number">0.3081</span>]</span><br><span class="line">	<span class="keyword">elif</span> args.dataset == <span class="string">&#x27;svhn&#x27;</span>:</span><br><span class="line">		mean, std = [<span class="number">0.4376821</span>, <span class="number">0.4437697</span>, <span class="number">0.47280442</span>], [<span class="number">0.19803012</span>, <span class="number">0.20101562</span>, <span class="number">0.19703614</span>]</span><br><span class="line">	<span class="keyword">elif</span> args.dataset == <span class="string">&#x27;cifar10&#x27;</span>:</span><br><span class="line">		mean = [x / <span class="number">255</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">125.3</span>, <span class="number">123.0</span>, <span class="number">113.9</span>]]</span><br><span class="line">		std = [x / <span class="number">255</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">63.0</span>, <span class="number">62.1</span>, <span class="number">66.7</span>]]</span><br><span class="line">	<span class="keyword">elif</span> args.dataset == <span class="string">&#x27;cifar100&#x27;</span>:</span><br><span class="line">		mean = [x / <span class="number">255</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">129.3</span>, <span class="number">124.1</span>, <span class="number">112.4</span>]]</span><br><span class="line">		std = [x / <span class="number">255</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">68.2</span>, <span class="number">65.4</span>, <span class="number">70.4</span>]]</span><br><span class="line">	<span class="keyword">elif</span> args.dataset == <span class="string">&#x27;imagenet32&#x27;</span>:</span><br><span class="line">		mean = [x / <span class="number">255</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">122.68</span>, <span class="number">116.66</span>, <span class="number">104.01</span>]]</span><br><span class="line">		std = [x / <span class="number">255</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">66.22</span>, <span class="number">64.20</span>, <span class="number">67.86</span>]]</span><br><span class="line"></span><br><span class="line">	transform = transforms.Compose([</span><br><span class="line">		transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">		transforms.ToTensor(),</span><br><span class="line">		transforms.Normalize(mean, std),</span><br><span class="line">	])</span><br><span class="line">	<span class="keyword">if</span> dataset == <span class="string">&#x27;mnist&#x27;</span>:</span><br><span class="line">		transform.transforms.append(transforms.Lambda(<span class="keyword">lambda</span> x: x.repeat(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">	<span class="keyword">return</span> transform</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">dataset, n_classes</span>):</span><br><span class="line">	data_label = &#123;i: [] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes)&#125;</span><br><span class="line">	<span class="keyword">for</span> x, y <span class="keyword">in</span> dataset:</span><br><span class="line">		data_label[y].append(x)</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">		data_label[i] = torch.stack(data_label[i])</span><br><span class="line">	</span><br><span class="line">	holder = &#123;i: [] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes)&#125;</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">		<span class="keyword">with</span> torch.no_grad():</span><br><span class="line">			data = feature_extractor(data_label[i])</span><br><span class="line">			holder[i].append(data.squeeze())</span><br><span class="line">	<span class="keyword">return</span> holder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImageNet32</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">	train_list = [</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_1&#x27;</span>, <span class="string">&#x27;27846dcaa50de8e21a7d1a35f30f0e91&#x27;</span>],</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_2&#x27;</span>, <span class="string">&#x27;c7254a054e0e795c69120a5727050e3f&#x27;</span>],</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_3&#x27;</span>, <span class="string">&#x27;4333d3df2e5ffb114b05d2ffc19b1e87&#x27;</span>],</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_4&#x27;</span>, <span class="string">&#x27;1620cdf193304f4a92677b695d70d10f&#x27;</span>],</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_5&#x27;</span>, <span class="string">&#x27;348b3c2fdbb3940c4e9e834affd3b18d&#x27;</span>],</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_6&#x27;</span>, <span class="string">&#x27;6e765307c242a1b3d7d5ef9139b48945&#x27;</span>],</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_7&#x27;</span>, <span class="string">&#x27;564926d8cbf8fc4818ba23d2faac7564&#x27;</span>],</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_8&#x27;</span>, <span class="string">&#x27;f4755871f718ccb653440b9dd0ebac66&#x27;</span>],</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_9&#x27;</span>, <span class="string">&#x27;bb6dd660c38c58552125b1a92f86b5d4&#x27;</span>],</span><br><span class="line">		[<span class="string">&#x27;train_data_batch_10&#x27;</span>, <span class="string">&#x27;8f03f34ac4b42271a294f91bf480f29b&#x27;</span>],</span><br><span class="line">	]</span><br><span class="line">	valid_list = [</span><br><span class="line">		[<span class="string">&#x27;val_data&#x27;</span>, <span class="string">&#x27;3410e3017fdaefba8d5073aaa65e4bd6&#x27;</span>],</span><br><span class="line">	]</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root, n_class, transform</span>):</span><br><span class="line">		self.transform = transform</span><br><span class="line">		downloaded_list = self.train_list</span><br><span class="line">		self.n_class = n_class</span><br><span class="line">		self.data_label = &#123;i: [] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_class)&#125;</span><br><span class="line">		self.data = []</span><br><span class="line">		self.targets = []</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span> i, (file_name, checksum) <span class="keyword">in</span> <span class="built_in">enumerate</span>(downloaded_list):</span><br><span class="line">			file_path = os.path.join(root, file_name)</span><br><span class="line">			<span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">				<span class="keyword">if</span> sys.version_info[<span class="number">0</span>] == <span class="number">2</span>:</span><br><span class="line">					entry = pickle.load(f)</span><br><span class="line">				<span class="keyword">else</span>:</span><br><span class="line">					entry = pickle.load(f, encoding=<span class="string">&#x27;latin1&#x27;</span>)</span><br><span class="line">				<span class="keyword">for</span> j, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(entry[<span class="string">&#x27;labels&#x27;</span>]):</span><br><span class="line">					self.data_label[k - <span class="number">1</span>].append(entry[<span class="string">&#x27;data&#x27;</span>][j])</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_class):</span><br><span class="line">			self.data_label[i] = np.vstack(self.data_label[i]).reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">			self.data_label[i] = self.data_label[i].transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))  <span class="comment"># convert to HWC</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, use_num_cls, max_num=<span class="literal">None</span></span>):</span><br><span class="line">		<span class="keyword">assert</span> <span class="built_in">isinstance</span>(use_num_cls, <span class="built_in">list</span>) \</span><br><span class="line">		       <span class="keyword">and</span> <span class="built_in">len</span>(use_num_cls) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">len</span>(use_num_cls) &lt; self.n_class, \</span><br><span class="line">			<span class="string">&#x27;invalid use_num_cls : &#123;:&#125;&#x27;</span>.<span class="built_in">format</span>(use_num_cls)</span><br><span class="line">		new_data, new_targets = [], []</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> use_num_cls:</span><br><span class="line">			new_data.append(self.data_label[i][:max_num] <span class="keyword">if</span> max_num <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.data_label[i])</span><br><span class="line">			new_targets.extend([i] * max_num <span class="keyword">if</span> max_num <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">			                   <span class="keyword">else</span> [i] * <span class="built_in">len</span>(self.data_label[i]))</span><br><span class="line">		self.data = np.concatenate(new_data)</span><br><span class="line">		self.targets = new_targets</span><br><span class="line">		</span><br><span class="line">		imgs = []</span><br><span class="line">		<span class="keyword">for</span> img <span class="keyword">in</span> self.data:</span><br><span class="line">			img = Image.fromarray(img)</span><br><span class="line">			img = self.transform(img)</span><br><span class="line">			<span class="keyword">with</span> torch.no_grad():</span><br><span class="line">				imgs.append(feature_extractor(img.unsqueeze(<span class="number">0</span>)).squeeze().unsqueeze(<span class="number">0</span>))</span><br><span class="line">		<span class="keyword">return</span> torch.cat(imgs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	ncls = &#123;<span class="string">&#x27;mnist&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;svhn&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;cifar10&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;cifar100&#x27;</span>: <span class="number">100</span>, <span class="string">&#x27;imagenet32&#x27;</span>: <span class="number">1000</span>&#125;</span><br><span class="line">	transform = get_transform(args.dataset)</span><br><span class="line">	<span class="keyword">if</span> args.dataset == <span class="string">&#x27;imagenet32&#x27;</span>:</span><br><span class="line">		imgnet32 = ImageNet32(args.data, ncls[args.dataset], transform)</span><br><span class="line">		data_label = &#123;i: [] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)&#125;</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">			m = imgnet32.get([i])</span><br><span class="line">			data_label[i].append(m)</span><br><span class="line">			<span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">				<span class="built_in">print</span>(<span class="string">f&#x27;Currently saving features of <span class="subst">&#123;i&#125;</span>-th class&#x27;</span>)</span><br><span class="line">				torch.save(data_label, <span class="string">f&#x27;<span class="subst">&#123;args.save_path&#125;</span>/<span class="subst">&#123;args.dataset&#125;</span>bylabel.pt&#x27;</span>)</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">if</span> args.dataset == <span class="string">&#x27;mnist&#x27;</span>:</span><br><span class="line">			data = dset.MNIST(args.data_path, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">		<span class="keyword">elif</span> args.dataset == <span class="string">&#x27;svhn&#x27;</span>:</span><br><span class="line">			data = dset.SVHN(args.data_path, split=<span class="string">&#x27;train&#x27;</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">		<span class="keyword">elif</span> args.dataset == <span class="string">&#x27;cifar10&#x27;</span>:</span><br><span class="line">			data = dset.CIFAR10(args.data_path, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">		<span class="keyword">elif</span> args.dataset == <span class="string">&#x27;cifar100&#x27;</span>:</span><br><span class="line">			data = dset.CIFAR100(args.data_path, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">		dataset = process(data, ncls[args.dataset])</span><br><span class="line">		torch.save(dataset, <span class="string">f&#x27;<span class="subst">&#123;args.save_path&#125;</span>/<span class="subst">&#123;args.dataset&#125;</span>bylabel.pt&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

]]></content>
  </entry>
</search>
